<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="" />
  <meta name="author" content="and contributors" />
   <title>Mínimos quadrados, maximização da verossimilhança e quantificação de incertezas em regressões lineares</title>  
  <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png"/>
  <link rel="stylesheet" href="/css/base.css"/>
  
  <script src="/libs/mousetrap/mousetrap.min.js"></script>

  
    <link rel="stylesheet" href="/libs/highlight/github.min.css">
    <script src="/libs/highlight/highlight.pack.js"></script>
    <script src="/libs/highlight/julia.min.js"></script>
    <script>
      document.addEventListener('DOMContentLoaded', (event) => {
        document.querySelectorAll('pre').forEach((el) => {
          hljs.highlightElement(el);
        });
      });
    </script>
  

  
    <link rel="stylesheet" href="/libs/katex/katex.min.css">
  
</head>

<body>

  <div class="books-container">

  <aside class="books-menu">
  <input type="checkbox" id="menu">
  <label for="menu">☰</label>

  <div class="books-title">
    <a href="/">Modelagem Matemática</a>
  </div>

  <br />

  <div class="books-subtitle">
    Notas de aula
  </div>

  <br />

  <div class="books-author">
    <a href="https://rmsrosa.github.io">Ricardo M. S. Rosa</a>
  </div>

  <div class="books-menu-content">
    <div class="menu-level-1">
    <li><a href="/pages/intro">Introdução</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE I</li>
    </div>
    <div class="menu-level-1">
    <li>1. Preliminares</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c01/0101-Aspectos_curso">1.1. Aspectos do curso</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c01/0102-Instalando_acessando_Julia">1.2. Instalando e acessando o Julia</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c01/0103-Primeiros_passos_Julia">1.3. Primeiros passos em Julia</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE II</li>
    </div>
    <div class="menu-level-1">
    <li>2. Princípios de Modelagem Matemática</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c02/0201-Principios_basicos">2.1. Princípios básicos de modelagem</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c02/0202-Exemplos_tipos_modelagem">2.2. Exemplos de tipos de modelagem</a></li>
    </div>
    <div class="menu-level-1">
    <li>3. Análise Dimensional</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c03/0301-Quantidades_unidades_dimensoes">3.1. Quantidades, unidades e dimensões</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c03/0302-BuckinghamPi">3.2. Análise dimensional e o Teorema de Buckingham-Pi</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c03/0303-Unidades_Julia">3.3. Trabalhando com unidades e dimensões em Julia</a></li>
    </div>
    <div class="menu-level-1">
    <li>4. Ajuste de Parâmetros</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0401-Minimos_quadrados_ajuste">4.1. Mínimos quadrados e o ajuste de parâmetros</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0402-Exemplos_ajuste_linear">4.2. Exemplos de ajuste linear</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0403-Modelos_redutiveis_linear_aplicacoes">4.3. Modelos redutíveis ao caso linear nos parâmetros e aplicações</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0404-Minimos_quadrados_nao_linear">4.4. Mínimos quadrados não-linear</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0405-Exemplos_ajuste_naolinear">4.5. Exemplos de ajuste não-linear de parâmetros</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0406-Redes_neurais">4.6. Redes neurais</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c04/0407-Ajuste_em_redes_neurais">4.7. Ajuste de parâmetros em modelos de redes neurais</a></li>
    </div>
    <div class="menu-level-1">
    <li>5. Erros e Incertezas</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c05/0501-Erros_e_incertezas">5.1. Erros e incertezas</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca">5.2. Mínimos quadrados, maximização da verossimilhança e quantificação de incertezas em regressões lineares</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c05/0503-Propagacao_incertezas">5.3. Propagação de incertezas</a></li>
    </div>
    <div class="menu-level-1">
    <li>6. Avaliação de Modelos</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c06/0601-Qualidade_do_modelo">6.1. Qualidade do ajuste</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c06/0602-Validacao_do_modelo">6.2. Validação de modelos</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c06/0603-Comparacao_de_modelos">6.3. Comparação de modelos</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE III</li>
    </div>
    <div class="menu-level-1">
    <li>7. Mecânica</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0701-Mecanica_Newtoniana">7.1. Mecânica Newtoniana</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0702-Mecanica_Lagrangiana">7.2. Mecânica Lagrangiana</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0703-Conservacao_contexto_Newtoniano">7.3. Leis de conservação em um contexto Newtoniano</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0704-Conservacao_contexto_Lagrangiano">7.4. Leis de conservação em um contexto Lagrangiano</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0705-Hamiltonianos">7.5. Hamiltonianos</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0706-Pendulo">7.6. Análise do período de um pêndulo planar simples</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c07/0707-Pendulo_angulos_grandes">7.7. Experimentos com pêndulos</a></li>
    </div>
    <div class="menu-level-1">
    <li>8. Modelos em Eletrônica</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c08/0801-Modelo_diodo">8.1. Modelagem da relação voltagem-corrente de um diodo</a></li>
    </div>
    <div class="menu-level-1">
    <li>9. Reações Químicas</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c09/0901-Lei_acao_de_massas">9.1. Lei de ação de massas</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c09/0902-Reacoes_enzimaticas">9.2. Modelagem de reações enzimática</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c09/0903-Isomerizacao">9.3. Lupulagem e a conversão de humulone em iso-humulone considerando saturação</a></li>
    </div>
    <div class="menu-level-1">
    <li>10. Modelos Epidemiológicos</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c10/1001-Modelos_epidemiologicos_compartimentais">10.1. Modelos epidemiológicos compartimentais</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c10/1002-Ajuste_SIR">10.2. Ajustando um modelo SIR a uma epidemia de Influenza</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c10/1003-Compartimentais_estruturados">10.3. Modelos compartimentais estruturados</a></li>
    </div>
    <div class="menu-level-1">
    <li>11. Séries de Fourier e Aplicações</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c11/1101-Series_Fourier">11.1. Séries de Fourier</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c11/1102-Transformada_discreta_Fourier">11.2. Transformada discreta de Fourier</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c11/1103-Ondas_sonoras_elementos_musicais">11.3. Ondas sonoras e elementos musicais</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/pages/jupytered/c11/1104-Compressao_audio">11.4. Séries de Fourier e compressão de audio</a></li>
    </div>
<div>


  
    <a href="https://github.com/rmsrosa/modelagem_matematica/tree/modmat2022p1"><img src="/assets/images/GitHub-Mark-32px.png" alt="GitHub repo" width="18" style="margin:5px 5px" align="left"></a>

  

</aside>


  <div class="books-content">

    
      <div class="navbar">
    <p id="nav">
<span id="nav-prev" style="float: left;">
<a class="menu-level-1" href="/pages/jupytered/c05/0501-Erros_e_incertezas">5.1. Erros e incertezas <kbd>←</kbd></a>
</span>
<span id="nav-next" style="float: right;">
    <a class="menu-level-1" href="/pages/jupytered/c05/0503-Propagacao_incertezas"><kbd>→</kbd> 5.3. Propagação de incertezas</a>
</span>
    </p>
</div>
</br></br>

    

    
      <div class="badges">
<p>
<a href="https://nbviewer.org/urls/rmsrosa.github.io/modelagem_matematica/generated/jupytered/c05/0502-Minimos_quadrados_verossimilhanca.ipynb"><img align="left" src="https://img.shields.io/badge/view%20in-nbviewer-orange" alt="View in NBViewer" title="View Jupyter notebook in NBViewer"></a>
<a href="https://mybinder.org/v2/gh/rmsrosa/modelagem_matematica/julia-env-for-binder-2022p1?urlpath=git-pull%3Frepo%3Dhttps://github.com/rmsrosa/modelagem_matematica%26urlpath%3Dlab/tree%252Fmodelagem_matematica/generated/jupytered/c05/0502-Minimos_quadrados_verossimilhanca.ipynb%26branch%3Dgh-pages"><img align="left" src="https://mybinder.org/badge.svg" alt="Open in binder" title="Open in binder"></a>
<a href="/generated/jupytered/c05/0502-Minimos_quadrados_verossimilhanca.ipynb"><img align="left" src="https://img.shields.io/badge/download-notebook-blue" alt="Download notebook" title="Download Jupyter notebook"></a>
<a href="/src/jupyter/c05/0502-Minimos_quadrados_verossimilhanca.ipynb"><img align="left" src="https://img.shields.io/badge/view-source-lightblue" alt="View source" title="View source"></a>
</p>
</div></br>

    
<h1 id="get_title"><a href="#get_title" class="header-anchor">5.2. Mínimos quadrados, maximização da verossimilhança e quantificação de incertezas em regressões lineares</a></h1>
<ul>
<li><p>Continuando o assunto de probabilidade, vamos ver o problema de mínimos quadrados linear como um problema de maximização da verossimilhança.</p>
</li>
<li><p>Os dois métodos são equivalentes, quando a distribuição de probabilidade do erro das amostras é normal.</p>
</li>
<li><p>Mas a verossimilhança envolve uma interpretação diferente do problema, com aspectos probabilísticos.</p>
</li>
<li><p>Como consequência, podemos quantificar as incertezas na escolha dos parâmetros e nas predições do modelo.</p>
</li>
</ul>
<pre><code class="language-julia">using Distributions
using Plots
using Random
using LinearAlgebra: ⋅</code></pre>
<h2 id="hipóteses_probabilísticas"><a href="#hipóteses_probabilísticas" class="header-anchor">Hipóteses probabilísticas</a></h2>
<ul>
<li><p>Consideremos um modelo linear </p>
</li>
</ul>
\[ y = \beta_0  + \beta_1 x
\]
<ul>
<li><p>De um ponto de vista probabilístico, consideramos incertezas inerentes na obtenção dos dados \(x\) e na definição do modelo.</p>
</li>
<li><p>Com base nisso, interpretamos o modelo como uma relação para o <strong>valor esperado</strong> de \(y\), <strong>dado</strong> \(x\), i.e.</p>
</li>
</ul>
\[ E(y|x) = \beta_0  + \beta_1 x
\]
<ul>
<li><p>Tanto \(y\) como o erro \(\epsilon = y - \beta_0  + \beta_1 x\) são vistos como variáveis aleatórias e assume-se que o erro segue uma normal, com o mesmo desvio padrão ao longo da variável \(x\):</p>
</li>
</ul>
\[ \epsilon \sim \mathcal{N}(0,\sigma^2).
\]
<ul>
<li><p>Assim, a probabilidade condicional de \(y\) dado \(x\) é</p>
</li>
</ul>
\[ \mathcal{P}(y|x) \sim \mathcal{N}(\beta_0 + \beta_1 x, \sigma^2).
\]
<ul>
<li><p>Além disso, assumimos que os erros são independentes entre si. Ou seja, em quaisquer dois pontos \(x_i, x_j\), os erros \(\epsilon_i, \epsilon_j\) nesses pontos são independentes entre si.</p>
</li>
<li><p>Como os erros são normais, serem independentes é equivalente a não estarem correlacionados, o que pode ser expresso por \(E(\epsilon_i\epsilon_j)=0\).</p>
</li>
</ul>
<h2 id="verossimilhança"><a href="#verossimilhança" class="header-anchor">Verossimilhança</a></h2>
<ul>
<li><p>A função densidade de probabilidade da normal \(\mathcal{N}(\mu, \sigma)\) é</p>
</li>
</ul>
\[ f_{\mu, \sigma^2}(s) = \frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}e^{\displaystyle -\frac{(s-\mu)^2}{2\sigma^2}}.
\]
<ul>
<li><p>No caso de \(\mathcal{P}(y|x) \sim \mathcal{N}(\beta_0 + \beta_1 x, \sigma^2)\), e considerando \(\sigma\) fixo, a função densidade de probabilidade de \(\mathcal{P}(y|x)\), que depende de \(\boldsymbol\beta = (\beta_0, \beta_1)\), se torna</p>
</li>
</ul>
\[ f_\beta(y) = \frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}e^{\displaystyle -\frac{(y - \beta_0 - \beta_1 x)^2}{2\sigma^2}}.
\]
<ul>
<li><p>Olhando essa função de maneira diferente, em um dado valor observado \(y\) &#40;para um certo \(x\) fixo&#41;, e com o parâmetro \(\boldsymbol\beta\) variável, temos a <strong>função de verossimilhança</strong></p>
</li>
</ul>
\[ \mathcal{L}_y(\boldsymbol\beta) = f_{\boldsymbol\beta}(y) = \frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}e^{\displaystyle -\frac{(y - \beta_0 - \beta_1 x)^2}{2\sigma^2}}.
\]
<ul>
<li><p>Observe que quanto mais perto \(\beta_0 + \beta_1 x\) estiver de \(y\), maior será a verossimilhança.</p>
</li>
<li><p>Mas independentemente da forma da função de densidade de distribuição, maximizar a verossimilhança é aumentar a probabilidade do modelo resultar no dado observado.</p>
</li>
</ul>
<h2 id="função_de_verossimilhança_em_um_conjunto_de_observações"><a href="#função_de_verossimilhança_em_um_conjunto_de_observações" class="header-anchor">Função de verossimilhança em um conjunto de observações</a></h2>
<ul>
<li><p>Dado um conjunto de observações &#40;independentes&#41; \((\mathbf{x}, \mathbf{y}) = (x_i, y_i)_{i=1}^N\), a probabilidade conjunta é o produto das probabilidades, \(\mathcal{P}(y_1|x_1)\mathcal{P}(y_2|x_2)\cdots\mathcal{P}(y_N|x_N)\).</p>
</li>
<li><p>Em relação à função densidade de probabilidades, obtemos</p>
</li>
</ul>
\[ f_N(\mathbf{y},\boldsymbol\beta) = \frac{1}{\displaystyle (2\pi \sigma^2)^{N/2}}e^{\displaystyle -\frac{1}{2\sigma^2}\sum_{i=1}^N(y_i - \beta_0 - \beta_1 x_i)^2}.
\]
<ul>
<li><p>A função de verossimilhança da amostra é</p>
</li>
</ul>
\[ \mathcal{L}_N(\boldsymbol\beta) = \Pi_i \mathcal{L}_{y_i}(\boldsymbol\beta) = f_N(\mathbf{y},\boldsymbol\beta) = \frac{1}{\displaystyle (2\pi \sigma^2)^{N/2}}e^{\displaystyle -\frac{1}{2\sigma^2}\sum_{i=1}^N(y_i - \beta_0 - \beta_1 x_i)^2}.
\]
<ul>
<li><p>Maximizar \(\mathcal{L}_N(\boldsymbol\beta)\) é aumentar a probabilidade do modelo resultar numa boa aproximação para o conjunto de dados observados.</p>
</li>
</ul>
<h2 id="função_de_log-verossimilhança"><a href="#função_de_log-verossimilhança" class="header-anchor">Função de log-verossimilhança</a></h2>
<ul>
<li><p>Obtivemos a função de verossimilhança &quot;pontual&quot;</p>
</li>
</ul>
\[ \mathcal{L}_y(\boldsymbol\beta) = \frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}e^{\displaystyle -\frac{(y - \beta_0 - \beta_1 x)^2}{2\sigma^2}}.
\]
<ul>
<li><p>Associada a ela, temos a <strong>função de log-verossimilhança</strong></p>
</li>
</ul>
\[ \ell_y(\boldsymbol\beta) = \log(\mathcal{L}_y(\boldsymbol\beta) = \log\left(\frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}\right) - \frac{(y - \beta_0 - \beta_1 x)^2}{2\sigma^2}.
\]
<ul>
<li><p>Como o logaritmo é monónoto crescente, maximizar a verossimilhança é equivalente a maximizar a log-verossimilhança.</p>
</li>
<li><p>No caso da normal, vemos que a log-verossimilhança tem uma dependência bem mais amigável, no sentido de ser linear.</p>
</li>
<li><p>No caso de um conjunto de dados, a função de log-verossimilhança toma a forma</p>
</li>
</ul>
\[ \ell_N(\boldsymbol\beta) = \log \mathcal{L}_N(\boldsymbol\beta) = N\log\left(\frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}\right) - \frac{1}{2\sigma^2}\sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_i)^2.
\]
<h2 id="maximização_da_verossimilhança_e_minimização_do_erro_quadrático"><a href="#maximização_da_verossimilhança_e_minimização_do_erro_quadrático" class="header-anchor">Maximização da verossimilhança e minimização do erro quadrático</a></h2>
<ul>
<li><p>Chegamos, então, a</p>
</li>
</ul>
\[ \ell_N(\boldsymbol\beta) = N\log\left(\frac{1}{\displaystyle \sqrt{2\pi \sigma^2}}\right) - \frac{1}{2\sigma^2}\sum_{i=1}^N (y - \beta_0 - \beta_1 x)^2.
\]
<ul>
<li><p>O primeiro termo não depende de \(\boldsymbol\beta\).</p>
</li>
<li><p>E vemos, do segundo termo, que maximizar a verossimilhança é equivalente a minimizar o erro quadrático &#40;i.e. a soma dos quadrados dos resíduos &#40;RSS&#41;&#41;</p>
</li>
</ul>
\[ \operatorname{RSS}(\boldsymbol\beta) = \sum_{i=1}^N (y - \beta_0 - \beta_1 x)^2.
\]
<ul>
<li><p>Dessa forma, estabelecemos a equivalência entre os dois métodos &#40;mas com interpretações diferentes&#41;:</p>
</li>
</ul>
\[ \hat{\boldsymbol\beta} = \operatorname{argmax}_{\boldsymbol\beta} \mathcal{L}_N(\boldsymbol\beta) = \operatorname{argmax}_{\boldsymbol\beta} \ell_N(\boldsymbol\beta) = \operatorname{argmin}_{\boldsymbol\beta} \operatorname{RSS}(\boldsymbol\beta).
\]
<ul>
<li><p>Essa equivalência vale quando os erros são normais independentes, com média zero, e desvio padrão uniforme ao longo de \(x\) &#40;ou seja, são normais i.i.d. com média zero&#41;.</p>
</li>
</ul>
<h2 id="estimativa_sobre_a_determinação_nos_parâmetros"><a href="#estimativa_sobre_a_determinação_nos_parâmetros" class="header-anchor">Estimativa sobre a determinação nos parâmetros</a></h2>
<ul>
<li><p>Com esse arcabouço probabilístico, podemos extrair informações sobre a incerteza na determinação dos parâmetros \(\boldsymbol\beta=(\beta_0, \beta_1)\). </p>
</li>
<li><p>Obtivemos, acima, o maximizador da verossimilhança \(\hat{\boldsymbol\beta}\) dado exatamente pela forma normal da solução pelo método de mínimos quadrados:</p>
</li>
</ul>
\[ \hat{\boldsymbol\beta} = (X^TX)^{-1}X^T\mathbf{y}.
\]
<ul>
<li><p>Nesta fórmula, \(X = [\mathbf{1}, \mathbf{x}]\) é a matrix de Vandermonde, que assumimos de posto máximo, e \(\mathbf{y}\) é dado por \(\mathbf{y} = (y_1, \ldots, y_N)\)</p>
</li>
<li><p>Agora, ao invés de olharmos estritamente para \(\boldsymbol\beta\) que minimiza o erro entre as medições \(y_i\) e o do resultado do modelo \(\beta_0 + \beta_1 x_i\), vamos olhar para possíveis outras escolhas \(\mathbf{y} = X\mathbf{\boldsymbol\beta} + \boldsymbol{\epsilon}\), que talvez extrapolassem melhor \(y = \beta_0 + \beta_1 x\) para outros dados.</p>
</li>
<li><p>Considerando, então, \(\mathbf{y} = X\mathbf{\boldsymbol\beta} + \boldsymbol{\epsilon}\), obtemos</p>
</li>
</ul>
\[ \hat{\boldsymbol\beta} = (X^TX)^{-1}X^T(X\boldsymbol\beta + \boldsymbol{\epsilon}) = \boldsymbol\beta + (X^TX)^{-1}X^T\boldsymbol{\epsilon}.
\]
<ul>
<li><p>Logo, \(\boldsymbol\beta\) é uma variável aleatória cuja diferença para o maximizador \(\hat{\boldsymbol\beta}\) é dada por</p>
</li>
</ul>
\[ \boldsymbol\beta - \hat{\boldsymbol\beta} = - (X^TX)^{-1}X^T\boldsymbol{\epsilon}.
\]
<h2 id="variância_na_determinação_dos_parâmetros"><a href="#variância_na_determinação_dos_parâmetros" class="header-anchor">Variância na determinação dos parâmetros</a></h2>
<ul>
<li><p>Como \(\hat{\boldsymbol\beta}\) está fixo &#40;constante&#41;, temos \(E(\hat{\boldsymbol\beta}) = \hat{\boldsymbol\beta}\), de modo que \(\operatorname{Var}(\boldsymbol\beta) = \operatorname{Var}(\boldsymbol\beta - \hat{\boldsymbol\beta}) = \operatorname{Var}(\hat{\boldsymbol\beta}-\boldsymbol\beta)\) &#40;verifique&#33;&#41;. Portanto,</p>
</li>
</ul>
\[ \operatorname{Var}(\boldsymbol\beta) = \operatorname{Var}\left((X^TX)^{-1}X^T\boldsymbol{\epsilon}\right).
\]
<ul>
<li><p>Se \(X\) e \(\epsilon\) fossem escalares, seria fácil deduzir que \(\operatorname{Var}(\beta) = (X^TX)^{-1}X^T\operatorname{Var}(\epsilon)X(X^TX)^{-1}\). Mas não é o caso.</p>
</li>
<li><p>No caso multidimensional, em que \(X\) é de fato uma matriz e \(\boldsymbol\epsilon\) e \(\boldsymbol\beta\) são vetores, temos o valor esperado nos dando um vetor com os valores esperados de cada coordenada e temos a variância sendo estendida a uma <strong>matrix de variância-covariância</strong>, nos dando não apenas a variação de cada coordenada, mas também uma variação conjunta entre cada par de coordenadas. E a variância de \(\boldsymbol\beta\) envolve tudo isso.</p>
</li>
</ul>
<h2 id="valor_esperado_e_variância-covariância_de_variáveis_aleatórias_multidimensionais"><a href="#valor_esperado_e_variância-covariância_de_variáveis_aleatórias_multidimensionais" class="header-anchor">Valor esperado e variância-covariância de variáveis aleatórias multidimensionais</a></h2>
<ul>
<li><p>Por exemplo, se \(\boldsymbol\beta=(\beta_1, \ldots, \beta_m)\) é uma variável aleatória multidimensional, então o <strong>valor esperado</strong> &#40;ou média&#41; é</p>
</li>
</ul>
\[ E(\boldsymbol\beta) = (E(\beta_1), \ldots, E(\beta_m)).
\]
<ul>
<li><p>Também podemos considerar a <strong>variância</strong> de cada coordenada, \(\operatorname{Var}(\beta_i) = E((\beta_i - E(\beta_i))^2)\), nos dando o vetor variância</p>
</li>
</ul>
\[ \operatorname{Var}(\boldsymbol\beta) = (\operatorname{Var}(\beta_1), \ldots, \operatorname{Var}(\beta_m)).
\]
<ul>
<li><p>Mas também é relevante levarmos em consideração a <strong>covariância</strong> entre as coordenadas,</p>
</li>
</ul>
\[\operatorname{Cov}(\beta_j, \beta_k) = E((\beta_j - E(\beta_j))(\beta_k - E(\beta_k))).
\]
<ul>
<li><p>Naturalmente, \(\operatorname{Cov}(\beta_i, \beta_i) = \operatorname{Var}(\beta_i)\).</p>
</li>
<li><p>Juntando os dois conceitos, temos a <strong>matriz de variância-covariância</strong></p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = \operatorname{Cov}.(\mathbf{\beta}, \mathbf{\beta}^T) = \left[ \begin{matrix} 
  \operatorname{Var}(\beta_1) & \operatorname{Cov}(\beta_1, \beta_2) & \ldots & \operatorname{Cov}(\beta_1, \beta_m) \\
  \operatorname{Cov}(\beta_2, \beta_1) & \operatorname{Var}(\beta_2)& \dots & \operatorname{Cov}(\beta_2, \beta_m) \\
  \vdots & \vdots & \vdots & \vdots \\
  \operatorname{Cov}(\beta_m, \beta_1) & \operatorname{Cov}(\beta_m, \beta_2) & \ldots & \operatorname{Var}(\beta_m)
\end{matrix}\right].
\]
<ul>
<li><p>De forma mais compacta,</p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = E((\boldsymbol\beta - E(\boldsymbol\beta))(\boldsymbol\beta - E(\boldsymbol\beta))^T).
\]
<h2 id="variância-covariância_na_determinação_dos_parâmetros"><a href="#variância-covariância_na_determinação_dos_parâmetros" class="header-anchor">Variância-covariância na determinação dos parâmetros</a></h2>
<ul>
<li><p>Estamos interessados, então, na matriz \(\operatorname{Cov}(\boldsymbol\beta)\), sabendo que \(\boldsymbol\beta - \hat{\boldsymbol\beta} = - (X^TX)^{-1}X^T\boldsymbol{\epsilon}\).</p>
</li>
<li><p>Nesse caso, usamos que</p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = \operatorname{Cov}(\boldsymbol\beta - \hat{\boldsymbol\beta}) = \operatorname{Cov}((X^TX)^{-1}X^T\boldsymbol{\epsilon}).
\]
<ul>
<li><p>É possível mostrar, com alguns cálculos algébricos, que</p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = (X^TX)^{-1}X^TE(\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T)X(X^TX)^{-1} - (X^TX)^{-1}X^TE(\boldsymbol{\epsilon})E(\boldsymbol{\epsilon})^TX(X^TX)^{-1}.
\]
<ul>
<li><p>Como \(E(\boldsymbol\epsilon)=0\), sobra</p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = (X^TX)^{-1}X^TE(\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T)X(X^TX)^{-1}.
\]
<ul>
<li><p>Finalmente, da hipótese de que os erros são independentes entre si, têm média zero, e têm o mesmo desvio padrão \(\sigma\), então \(E(\epsilon_j\epsilon_k) = 0\), para \(j\neq k\), e \(E(\epsilon_j\epsilon_j) = \sigma^2\), ou seja</p>
</li>
</ul>
\[ E(\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T) = \sigma^2 I,
\]
<p>onde \(I\) é a matriz identidade.</p>
<ul>
<li><p>Logo,</p>
</li>
</ul>
\[ \operatorname{Cov}(\boldsymbol\beta) = \sigma^2(X^TX)^{-1}.
\]
<ul>
<li><p>Em particular, o vetor variância é a diagonal da matriz:</p>
</li>
</ul>
\[ \operatorname{Var}(\boldsymbol\beta) = \sigma^2\operatorname{diag}((X^TX)^{-1}).
\]
<h3 id="incerteza_do_modelo"><a href="#incerteza_do_modelo" class="header-anchor">Incerteza do modelo</a></h3>
<ul>
<li><p>Com a incerteza nos parâmetros caracterizada pela matriz de variância-covariância \(\operatorname{Cov}(\boldsymbol\beta)\), podemos estimar a incerteza do modelo</p>
</li>
<li><p>Considerando \(y = \beta_0 + \beta_1 x\), temos</p>
</li>
</ul>
\[ \operatorname{Var}(y) = \operatorname{Var}(\beta_0 + \beta_1 x).
\]
<ul>
<li><p>A questão é como calcular essa variância do lado direito da expressão.</p>
</li>
<li><p>Explicitando a expressão e usando que \(E(\beta_0 + \beta_1 x) = \hat\beta_0 + \hat\beta_1 x\), temos</p>
</li>
</ul>
\[ \operatorname{Var}(y)
= E\left( (\beta_0 + \beta_1 x - \hat\beta_0 - \hat\beta_1 x)^2\right)
= E\left( (\beta_0 - \hat\beta_0)^2 + 2(\beta_0 - \hat\beta_0)(\beta_1 - \hat\beta_1)x + (\beta_1 - \hat\beta_1)^2x^2)\right) \\
= \operatorname{Var}(\beta_0) + 2\operatorname{Cov}(\beta_0,\beta_1)x + \operatorname{Var}(\beta_1)x^2.
\]
<ul>
<li><p>Em forma matricial, e fazendo \(\mathbf{x} = (1,x)\), isso pode ser escrito como</p>
</li>
</ul>
\[ \operatorname{Var}(y) = \mathbf{x} \cdot \operatorname{Cov}(\boldsymbol \beta) \mathbf{x} = \sigma^2 \mathbf{x} \cdot (X^TX)^{-1}\mathbf{x}.
\]
<ul>
<li><p>Isso nos dá a variância de \(y = \beta_0 + \beta_1 x = \boldsymbol\beta \mathbf{x}\), em cada ponto \(x\).</p>
</li>
</ul>
<h3 id="intervalo_de_confiança"><a href="#intervalo_de_confiança" class="header-anchor">Intervalo de confiança</a></h3>
<ul>
<li><p>O processo de determinação dos parâmetros envolve uma média que faz com que variância calculada acima seja, de fato, uma medida da incerteza no parâmetro.</p>
</li>
<li><p>De fato, observemos o caso mais simples em que queremos modelar apenas os erros \(\epsilon = N(0,\sigma)\) por uma constante.</p>
</li>
<li><p>Fazemos uma série de amostras \((\epsilon_i)_{i=1}^N\) e buscamos encontrar o valor médio \(\hat\beta_0\) pelo método de mínimos quadrados.</p>
</li>
<li><p>Nesse caso, a matrix \(X\) é uma matrix \(N\times 1\) com todos os elementos iguais a 1:</p>
</li>
</ul>
\[ X = \left[ \begin{matrix} 1 \\ \vdots \\ 1 \end{matrix} \right]
\]
<ul>
<li><p>E o valor médio é dado por \(\hat\beta_0 = (X^TX)^{-1}(X^T\epsilon_i).\)</p>
</li>
</ul>
<ul>
<li><p>Observe que \(X^TX = N\) e \(X_T\epsilon_i = \sum_{i=1}^N \epsilon_i\), de modo que \(\hat\beta_0\) é, conforme esperado, o valor médio da amostra:</p>
</li>
</ul>
\[ \hat\beta_0 = \frac{1}{N}\sum_{i=1}^N \epsilon_i.
\]
<ul>
<li><p>Quanto à variância, obtemos, de fato, o erro padrão</p>
</li>
</ul>
\[ \operatorname{Var}(\beta) = \sigma^2 \operatorname{diag}((X^TX)^{-1}) = \frac{\sigma^2}{N}.
\]
<ul>
<li><p>Assim, os intervalos de confiânça para \(E(\beta)\) são dados em função de \(\operatorname{Var}(\beta)\), e.g.</p>
</li>
</ul>
\[ \operatorname{IC}_{68\%} = [\hat\beta_0 - \operatorname{Var}(\beta), \hat\beta_0 + \operatorname{Var}(\beta)], \qquad \operatorname{IC}_{95\%} = [\hat\beta_0 - 2\operatorname{Var}(\beta), \hat\beta_0 + 2\operatorname{Var}(\beta)].
\]
<ul>
<li><p>Caso sejam poucas amostras, devemos considerar a função t de Student no cálculo do fator multiplicativo de \(\operatorname{Var}(\beta)\) na obtenção dos intervalos de confiança.</p>
</li>
</ul>
<h3 id="desvio_padrão_corrigido_da_amostra"><a href="#desvio_padrão_corrigido_da_amostra" class="header-anchor">Desvio padrão corrigido da amostra</a></h3>
<ul>
<li><p>Observe que essas medidas dependem do desvio padrão \(\sigma\) da incerteza na coleta dos dados.</p>
</li>
<li><p>Em certos casos, essa medida pode ser obtida dos instrumentos de medição e do próprio processo de coleta.</p>
</li>
<li><p>Na falta de maiores informações, uma alternativa é utilizar o desvio padrão corrigido da amostra. No caso, no entanto, não estamos fazendo um conjunto \((x_j,y_j)\) de medições na mesma situação. Estamos medindo em pontos diversos. </p>
</li>
<li><p>Por conta disso, usamos o próprio modelo para compensar isso e devemos considerar que agora o grau de liberdade disponível é \(N-m\), onde \(m=2\).</p>
</li>
<li><p>No caso de dados nas &quot;mesmas&quot; condições, temos \(m=1\), como feito antes. Mas agora, temos dois parâmetros, \(\beta_0\) e \(\beta_1\), logo \(m=2\). Em outros modelos, \(m\) pode ser maior.</p>
</li>
<li><p>Assim, o <em>desvio padrão corrigido da amostra</em> é</p>
</li>
</ul>
\[ s_q^2 = \frac{1}{N-m}\sum_{i=1}^N (y_i - \hat{\boldsymbol\beta}\mathbf{x}_i)^2 = \frac{1}{N-2}\sum_{i=1}^N (y_i - \hat\beta_0 - \hat\beta_1x_i)^2.
\]
<h2 id="exemplo_sintético"><a href="#exemplo_sintético" class="header-anchor">Exemplo sintético</a></h2>
<ul>
<li><p>Vamos construir um exemplo sintético.</p>
</li>
<li><p>Vamos &quot;perturbar&quot; uma determinada reta com erros aleatórios segundo uma determinada normal com desvio padrão pré-definido.</p>
</li>
</ul>
<pre><code class="language-julia">b &#61; 1.0
m &#61; 0.2
σ &#61; 1.0
N &#61; 20</code></pre>
<pre><code class="language-julia">20</code></pre>
<pre><code class="language-julia">Random.seed&#33;&#40;1200&#41;
x_org &#61; &#91;0.0, 10.0&#93;
y_org &#61; b .&#43; m * x_org
data_x &#61; collect&#40;range&#40;0.0,10.0, length&#61;N&#41;&#41; &#43; rand&#40;N&#41;/N
data_y &#61; b .&#43; m * data_x .&#43; rand&#40;Normal&#40;0,σ&#41;, N&#41;
plot&#40;x_org, y_org, label &#61; &quot;y &#61; b &#43; mx&quot;, title &#61; &quot;Dados perturbados aleatoriamente, em um exemplo sintético&quot;, titlefont &#61; 10&#41;
scatter&#33;&#40;data_x, data_y, markersize&#61;3, label&#61;&quot;amostra&quot;&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_3_1.png" alt="">
<h3 id="determinando_os_parâmetros"><a href="#determinando_os_parâmetros" class="header-anchor">Determinando os parâmetros</a></h3>
<ul>
<li><p>Montamos a matriz de Vandermonde e resolvemos o problema de mínimos quadrados linear.</p>
</li>
</ul>
<pre><code class="language-julia">X &#61; &#91;ones&#40;N&#41; data_x&#93;
β &#61; X \ data_y</code></pre>
<pre><code class="language-julia">2-element Vector&#123;Float64&#125;:
 1.1988117103655653
 0.1431233525841148</code></pre>
<h3 id="visualizando_o_resultado"><a href="#visualizando_o_resultado" class="header-anchor">Visualizando o resultado</a></h3>
<pre><code class="language-julia">plot&#40;x_org, y_org, label&#61;&quot;original&quot;, legend&#61;:topleft, title &#61; &quot;Ajuste aos dados&quot;, titlefont &#61; 10&#41;
scatter&#33;&#40;data_x, data_y, markersize&#61;3, label&#61;&quot;amostra&quot;&#41;
plot&#33;&#40;x_org, β&#91;1&#93; .&#43; β&#91;2&#93; * x_org, label&#61;&quot;modelo&quot;&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_5_1.png" alt="">
<h3 id="intervalos_de_confiança_dos_parâmetros"><a href="#intervalos_de_confiança_dos_parâmetros" class="header-anchor">Intervalos de confiança dos parâmetros</a></h3>
<pre><code class="language-julia">Cov_β &#61; σ^2 * inv&#40;X&#39; * X&#41;</code></pre>
<pre><code class="language-julia">2×2 Matrix&#123;Float64&#125;:
  0.187317   -0.0273258
 -0.0273258   0.0054378</code></pre>
<pre><code class="language-julia">println&#40;&quot;Valores originais: β₀&#61;&#36;b, β₁&#61;&#36;m&quot;&#41;
println&#40;&quot;CI 68&#37; para β₀: &#91;&#36;&#40;round&#40;β&#91;1&#93; - Cov_β&#91;1,1&#93;,digits&#61;2&#41;&#41;, &#36;&#40;round&#40;β&#91;1&#93; &#43; Cov_β&#91;1,1&#93;,digits&#61;2&#41;&#41;&#93;&quot;&#41;
println&#40;&quot;CI 68&#37; para β₁: &#91;&#36;&#40;round&#40;β&#91;2&#93; - Cov_β&#91;2,2&#93;,digits&#61;2&#41;&#41;, &#36;&#40;round&#40;β&#91;2&#93; &#43; Cov_β&#91;2,2&#93;,digits&#61;2&#41;&#41;&#93;&quot;&#41;
println&#40;&quot;CI 95&#37; para β₀: &#91;&#36;&#40;round&#40;β&#91;1&#93; - 2Cov_β&#91;1,1&#93;,digits&#61;2&#41;&#41;, &#36;&#40;round&#40;β&#91;1&#93; &#43; 2Cov_β&#91;1,1&#93;,digits&#61;2&#41;&#41;&#93;&quot;&#41;
println&#40;&quot;CI 95&#37; para β₁: &#91;&#36;&#40;round&#40;β&#91;2&#93; - 2Cov_β&#91;2,2&#93;,digits&#61;2&#41;&#41;, &#36;&#40;round&#40;β&#91;2&#93; &#43; 2Cov_β&#91;2,2&#93;,digits&#61;2&#41;&#41;&#93;&quot;&#41;</code></pre>
<pre><code class="language-julia">Valores originais: β₀&#61;1.0, β₁&#61;0.2
CI 68&#37; para β₀: &#91;1.01, 1.39&#93;
CI 68&#37; para β₁: &#91;0.14, 0.15&#93;
CI 95&#37; para β₀: &#91;0.82, 1.57&#93;
CI 95&#37; para β₁: &#91;0.13, 0.15&#93;</code></pre>
<h3 id="variância_do_modelo_e_intervalos_de_confiança"><a href="#variância_do_modelo_e_intervalos_de_confiança" class="header-anchor">Variância do modelo e intervalos de confiança</a></h3>
<pre><code class="language-julia">Var_y &#61; &#91;&#91;1; x&#93; ⋅ &#40;Cov_β * &#91;1; x&#93;&#41; for x in data_x&#93;</code></pre>
<pre><code class="language-julia">20-element Vector&#123;Float64&#125;:
 0.18506906749986532
 0.1598972882552096
 0.13472605333885954
 0.1138367091305883
 0.09548818390233753
 0.08014032619999441
 0.0683046673239387
 0.059364145036716144
 0.053257487510014974
 0.05035707669772974
 0.05032645779142901
 0.053546214207226044
 0.059513423230789925
 0.06796962535835986
 0.08049659618144059
 0.09529307440702718
 0.11444701290418924
 0.13374704790809286
 0.15834618382131746
 0.18587335929487245</code></pre>
<pre><code class="language-julia">plot&#40;x_org, y_org, label&#61;&quot;original&quot;, size&#61;&#40;600,300&#41;, titlefont&#61;10,
    title&#61;&quot;Reta original, amostra, modelo, CI 68&#37; &#40;sombreado&#41;, CI 95&#37; &#40;barras de erro&#41;&quot;, &#41;
scatter&#33;&#40;data_x, data_y, markersize&#61;3, label&#61;&quot;amostra&quot;&#41;
plot&#33;&#40;data_x,β&#91;1&#93; .&#43; β&#91;2&#93; * data_x, grid&#61;false, yerror&#61;2*sqrt.&#40;Var_y&#41;,
    ribbon&#61;sqrt.&#40;Var_y&#41;, fillalpha&#61;0.2, label&#61;&quot;modelo&quot;, legend&#61;:topleft&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_9_1.png" alt="">
<h3 id="utilizando_o_desvio_padrão_corrigido_da_amostra"><a href="#utilizando_o_desvio_padrão_corrigido_da_amostra" class="header-anchor">Utilizando o desvio padrão corrigido da amostra</a></h3>
<ul>
<li><p>Como a amostra foi obtida de maneira sintética, conhecemos o desvio padrão associado a distribuição dos erros.</p>
</li>
<li><p>Mas em geral isso não está disponível.</p>
</li>
<li><p>Nesse caso, podemos aproximá-lo com o desvio padrão corrigido \(s_q\) da amostra.</p>
</li>
<li><p>Observe que \(s_q\) fica bem próximo do valor original de \(\sigma\) e que o desvio padrão não-corrigido da amostra fica ligeiramente distante.</p>
</li>
</ul>
<pre><code class="language-julia">s_q &#61; √&#40;sum&#40;abs2,  data_y .- β&#91;1&#93; .- β&#91;2&#93; * data_x&#41;/&#40;N-2&#41;&#41;
println&#40;&quot;Desvio padrão original: &#36;σ&quot;&#41;
println&#40;&quot;Desvio padrão corrigido da amostra: &#36;&#40;round&#40;s_q, digits&#61;3&#41;&#41;&quot;&#41;
println&#40;&quot;Desvio padrão não corrigido da amostra: &#36;&#40;round&#40;s_q * √&#40;&#40;N-2&#41;/N&#41;,digits&#61;3&#41;&#41;&quot;&#41;</code></pre>
<pre><code class="language-julia">Desvio padrão original: 1.0
Desvio padrão corrigido da amostra: 0.953
Desvio padrão não corrigido da amostra: 0.904</code></pre>
<pre><code class="language-julia">Cov_q &#61; s_q^2 * inv&#40;X&#39; * X&#41;
Var_q &#61; &#91;&#91;1; x&#93; ⋅ &#40;Cov_q * &#91;1; x&#93;&#41; for x in data_x&#93;</code></pre>
<pre><code class="language-julia">20-element Vector&#123;Float64&#125;:
 0.16813890537256826
 0.14526984645498642
 0.12240128207042794
 0.10342290001783515
 0.0867529022231941
 0.07280907017850197
 0.06205614017962694
 0.05393349898874427
 0.04838547993890799
 0.045750399394674124
 0.045722581513173675
 0.04864779384947513
 0.054069121178268606
 0.06175174793241178
 0.07313274849766525
 0.08657564139072518
 0.10397737305768065
 0.12151183628833889
 0.14386063742207578
 0.16886962036367678</code></pre>
<pre><code class="language-julia">plot&#40;x_org, y_org, label&#61;&quot;original&quot;, size&#61;&#40;600,300&#41;, titlefont&#61;10,
    title&#61;&quot;Reta original, amostra, modelo, CI 68&#37; &#40;sombreado&#41;, CI 95&#37; &#40;barras de erro&#41;\nusando o desvio padrão corrigido&quot;, &#41;
scatter&#33;&#40;data_x, data_y, markersize&#61;3, label&#61;&quot;amostra&quot;&#41;
plot&#33;&#40;data_x,β&#91;1&#93; .&#43; β&#91;2&#93; * data_x, grid&#61;false, yerror&#61;2*sqrt.&#40;Var_q&#41;,
    ribbon&#61;sqrt.&#40;Var_q&#41;, fillalpha&#61;0.2, label&#61;&quot;modelo&quot;, legend&#61;:topleft&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_12_1.png" alt="">
<h2 id="exemplo_salário_e_grau_de_instrução"><a href="#exemplo_salário_e_grau_de_instrução" class="header-anchor">Exemplo salário e grau de instrução</a></h2>
<ul>
<li><p>Vamos, agora, refazer o exemplo da relação entre o salário anual médio e o grau de instrução, dos EUA.</p>
</li>
<li><p>Dados obtidos de <a href="https://www.bls.gov/careeroutlook/2020/data-on-display/education-pays.htm">US Bureau of Labor Statistics: Learn more, earn more: Education leads to higher wages, lower unemployment</a>.</p>
</li>
</ul>
<table><tr><th align="right">Nível de instrução</th><th align="right">Média de salário semanal &#40;USD&#41;</th><th align="right">Taxa de desemprego &#40;&#37;&#41;</th></tr><tr><td align="right">Doutorado</td><td align="right">1883</td><td align="right">1,1</td></tr><tr><td align="right">Profissional</td><td align="right">1861</td><td align="right">1,6</td></tr><tr><td align="right">Mestrado</td><td align="right">1497</td><td align="right">2,0</td></tr><tr><td align="right">Graduação</td><td align="right">1248</td><td align="right">2,2</td></tr><tr><td align="right">Associado*</td><td align="right">887</td><td align="right">2,7</td></tr><tr><td align="right">Graduação incompleta</td><td align="right">833</td><td align="right">3.3</td></tr><tr><td align="right">Ensino Médio</td><td align="right">746</td><td align="right">3,7</td></tr><tr><td align="right">Ensino Fundamental</td><td align="right">592</td><td align="right">5,4</td></tr></table>
<ul>
<li><p><em>Associado</em> é um grau conferido em algumas instituições de nível superior, em cursos de dois a três anos.</p>
</li>
</ul>
<pre><code class="language-julia">data_y &#61; &#91;592, 746, 833, 887, 1248, 1497, 1861, 1883&#93;
plot&#40;data_y, seriestype &#61; :scatter, xlims&#61;&#40;0,9&#41;, ylims&#61;&#40;0,2000&#41;,
    xticks&#61;0:9, xaxis &#61; &quot;nível de instrução&quot;, yaxis&#61;&quot;salário &#40;USD&#41;&quot;, 
    title&#61;&quot;Média salarial semanal em função do grau de instrução&quot;, 
    titlefont&#61;12, legend&#61;false&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_13_1.png" alt="">
<pre><code class="language-julia">N &#61; 8
data_x &#61; collect&#40;1:N&#41;
X &#61; &#91;ones&#40;N&#41; data_x&#93;
β &#61; X\data_y
s_q &#61; √&#40;sum&#40;abs2,  data_y .- β&#91;1&#93; .- β&#91;2&#93; * data_x&#41;/&#40;N-2&#41;&#41;
Cov_q &#61; s_q^2 * inv&#40;X&#39; * X&#41;
Var_q &#61; &#91;&#91;1; x&#93; ⋅ &#40;Cov_q * &#91;1; x&#93;&#41; for x in data_x&#93;</code></pre>
<pre><code class="language-julia">8-element Vector&#123;Float64&#125;:
 6169.9875992063535
 4054.563279478461
 2644.280399659866
 1939.1389597505686
 1939.1389597505686
 2644.2803996598677
 4054.5632794784633
 6169.987599206355</code></pre>
<ul>
<li><p>Observe que não há muita diferença ao usarmos o fator da distribuição de Student no caso do intervalo de confiança de 68\&#37;, mas há uma diferença razoável no de 95\&#37;.</p>
</li>
</ul>
<pre><code class="language-julia">fator &#61; &#91;quantile&#40;TDist&#40;N-2&#41;,q&#41; for q &#61; &#40;0.84, 0.975&#41;&#93;</code></pre>
<pre><code class="language-julia">2-element Vector&#123;Float64&#125;:
 1.0839756791279644
 2.446911851144969</code></pre>
<pre><code class="language-julia">plot&#40;data_y, seriestype &#61; :scatter, xlims&#61;&#40;0,N&#43;1&#41;, ylims&#61;&#40;0,2000&#41;, color&#61;2,
    xticks&#61;0:N&#43;1, xaxis &#61; &quot;nível de instrução&quot;, yaxis&#61;&quot;salário &#40;USD&#41;&quot;, 
    label&#61;&quot;salário&quot;, title&#61;&quot;Média salarial semanal em função do grau de instrução&quot;, 
    titlefont&#61;12, legend&#61;:topleft&#41;
plot&#33;&#40;data_x,β&#91;1&#93; .&#43; β&#91;2&#93; * data_x, grid&#61;false, yerror&#61;fator&#91;2&#93;*sqrt.&#40;Var_q&#41;,
    ribbon&#61;fator&#91;1&#93;*sqrt.&#40;Var_q&#41;, fillalpha&#61;0.2, label&#61;&quot;modelo&quot;, color&#61;3, legend&#61;:topleft&#41;</code></pre>
<img src="/assets/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca/code/images/0502-Minimos_quadrados_verossimilhanca_16_1.png" alt="">
<ul>
<li><p><strong>Observação:</strong> Devo ressaltar, novamente, que o eixo &quot;nível de instrução&quot; não está associado a uma unidade de medida relevante, sendo o posicioamento dos dados um tanto arbitrário. Portanto, o resultado acima deve ser visto com cautela, apenas para efeitos ilustrativos.</p>
</li>
</ul>
<h2 id="exercícios"><a href="#exercícios" class="header-anchor">Exercícios</a></h2>
<ol>
<li><p>Faça as contas de que </p>
</li>
</ol>
\[ \operatorname{Cov}(\boldsymbol\beta) = \operatorname{Cov}(\boldsymbol\beta - \hat{\boldsymbol\beta}) = \operatorname{Cov}((X^TX)^{-1}X^T\boldsymbol{\epsilon}).
\]
<ol start="2">
<li><p>Obtenha \(\operatorname{Var}(\boldsymbol\beta)\) explicitamente em termos de \(N\) e de \((x_i)_{i=1}^N\).</p>
</li>
<li><p>No caso de apenas duas amostras, com \(x_2 - x_1 = d\) denotando a distância entre os dois pontos/momentos/condições de medição, escreva \(\operatorname{Var}(\boldsymbol\beta)\) explicitamente em função de \(d\) e \(x_1\), observe a influência da distância \(d\) na variância e encontre os limites dessa variância quanto \(d\searrow 0\) e \(d\nearrow \infty\).</p>
</li>
<li><p>Refaça os exercícios do caderno &quot;Modelos redutíveis ao caso linear nos parâmetros e aplicações&quot;, calculando e exibindo os intervalos de confiança em relação às variáveis usadas para linearizar os problemas.</p>
</li>
</ol>
<h2 id="referências"><a href="#referências" class="header-anchor">Referências</a></h2>
<ul>
<li><p>Morris H. DeGroot, Mark J Schervish, &quot;Probability and Statistics&quot;, Pearson Education  2012.</p>
</li>
<li><p>John R. Taylor, An Introduction to Error Analysis. The Study of Uncertainties in Physical Measurements. University Science Books, 1997.</p>
</li>
</ul>

    <div class="navbar">
    <p id="nav">
<span id="nav-prev" style="float: left;">
<a class="menu-level-1" href="/pages/jupytered/c05/0501-Erros_e_incertezas">5.1. Erros e incertezas <kbd>←</kbd></a>
</span>
<span id="nav-next" style="float: right;">
    <a class="menu-level-1" href="/pages/jupytered/c05/0503-Propagacao_incertezas"><kbd>→</kbd> 5.3. Propagação de incertezas</a>
</span>
    </p>
</div>
</br></br>



<div class="page-foot">
    
        <div class="license">
            <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/>(CC BY-NC-ND 4.0) Attribution-NonCommercial-NoDerivatives 4.0 International </a>
            
        </div>
    

    Last modified: June 21, 2022. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
</div><!-- CONTENT ENDS HERE -->

      </div> <!-- .books-content -->
    </div> <!-- .books-container -->

    
        <script src="/libs/katex/katex.min.js"></script>
        <script src="/libs/katex/auto-render.min.js"></script>
        <script>renderMathInElement(document.body)</script>
    

    
        <script src="/libs/highlight/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>
    

  </body>
</html>
