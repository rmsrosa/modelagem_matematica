{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--HEADER-->\n",
    "*Notas de aula de Modelagem Matemática - 2020/2 [- Ricardo M. S. Rosa (IM/UFRJ)](http://www.im.ufrj.br/rrosa)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--BADGES-->\n",
    "<a href=\"https://nbviewer.jupyter.org/github/rmsrosa/modelagem_matematica/blob/modmat2020p2/notas_de_aula/09.00-Minimos_quadrados_nao_linear.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://img.shields.io/badge/view%20in-nbviewer-orange\" alt=\"View in NBViewer\" title=\"View in NBViewer\"></a><a href=\"https://mybinder.org/v2/gh/rmsrosa/modelagem_matematica/julia-env-for-binder?urlpath=git-pull%3Frepo%3Dhttps%253A%252F%252Fgithub.com%252Frmsrosa%252Fmodelagem_matematica%26branch%3Dmodmat2020p2%26urlpath%3Dtree%252Fmodelagem_matematica%252Fnotas_de_aula/09.00-Minimos_quadrados_nao_linear.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in binder\" title=\"Open in binder\"></a><a href=\"https://nbviewer.jupyter.org/github/rmsrosa/modelagem_matematica/blob/modmat2020p2/notas_de_aula/slides/09.00-Minimos_quadrados_nao_linear.slides.html\" target=\"_blank\"><img align=\"left\" src=\"https://img.shields.io/badge/view-slides-darkgreen\" alt=\"View Slides\" title=\"View Slides\"></a>&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--NAVIGATOR-->\n",
    "[<- 8. Modelos redutíveis ao caso linear nos parâmetros e aplicações](08.00-Modelos_redutiveis_linear_aplicacoes.ipynb) | [Página inicial](00.00-Pagina_inicial.ipynb) \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mínimos quadrados não-linear\n",
    "\n",
    "* Modelos reais são raramente lineares.\n",
    "\n",
    "* E nem todos os fenômenos lineares podem ser bem aproximados por modelos redutíveis a lineares, como vimos da última vez.\n",
    "\n",
    "* Como ajustar parâmetros de modelos genuinamente não lineares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contexto\n",
    "\n",
    "* Considere um conjunto de dados $(x_i, y_i)$ com $x_i, y_i \\in \\mathbb{R}$, $i = 1, \\ldots, N$.\n",
    "\n",
    "* Buscamos aproximar o fenômeno com um modelo da forma\n",
    "$$y(t) =  \\varphi(x, \\boldsymbol{\\beta}),$$\n",
    "onde $\\boldsymbol{\\beta} = (\\beta_1, \\ldots, \\beta_m)$ é um conjunto de parâmetros (desconhecidos) do modelo.\n",
    "\n",
    "* Idealmente, poderíamos ajustar os parâmetros por **colocação**, ou seja, de tal forma que eles coincidissem em todos os dados:\n",
    "$$y(t_i) =  \\varphi(x_i ; \\theta), \\quad i=1, \\ldots, N.$$\n",
    "\n",
    "* No entanto, assim como no caso linear, não esperamos essa \"perfeição\" em geral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### O problema de mínimos quadrados não linear\n",
    "\n",
    "* Buscamos, então, *reduzir o erro*, em algum sentido. Por exemplo, o de minimizar o **erro quadrático**\n",
    "$$ E(\\boldsymbol{\\beta}) = \\sum_{i=1}^N |y_i - \\varphi(x_i,\\boldsymbol{\\beta})|^2.\n",
    "$$\n",
    "\n",
    "* Cada componente é, novamente, chamado de **resíduo** (não-linear)\n",
    "$$ r_i = y_i - \\varphi(x_i,\\boldsymbol{\\beta}), \\qquad i=1, \\ldots, N.\n",
    "$$\n",
    "\n",
    "* Assim, chegamos ao problema de **otimização**\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\sum_{i=1}^N |y_i - \\varphi(x_i,\\boldsymbol{\\beta})|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Condição para ser ponto de mínimo\n",
    "\n",
    "* Uma condição **necessária** para que $\\hat{\\boldsymbol\\beta}$ seja um ponto de mínimo de $E(\\boldsymbol\\beta)$ é que ele seja um **ponto crítico:**\n",
    "$$\\nabla E(\\hat{\\boldsymbol\\beta}) = 0.\n",
    "$$\n",
    "\n",
    "* Em termos de coordenadas, é necessário que\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} E(\\hat{\\boldsymbol\\beta}) = 0, \\qquad \\forall j=1, \\ldots, m.\n",
    "$$\n",
    "\n",
    "* Calculando explicitamente, temos\n",
    "$$ \\frac{\\partial}{\\partial \\beta_j} E(\\hat{\\boldsymbol\\beta}) =   \\frac{\\partial}{\\partial \\beta_j}\\left( \\sum_{i=1}^N \\left|y_i - \\varphi(x_i, \\boldsymbol\\beta)\\right|^2 \\right) =  \\sum_{i=1}^N (y_i - \\varphi(x_i, \\boldsymbol\\beta))\\frac{\\partial}{\\partial \\beta_j}\\varphi(x_i, \\boldsymbol{\\beta}) = 0,\n",
    "$$\n",
    "para $j=1, \\ldots, m.$\n",
    "\n",
    "* Em termos matriciais, podemos escrever\n",
    "$$D\\mathbf{r}(\\hat{\\boldsymbol\\beta})^t \\mathbf{r}(\\hat{\\boldsymbol\\beta}) = 0. \\quad\\quad (2)\n",
    "$$\n",
    "onde $D\\mathbf{r}(\\boldsymbol\\beta)^t$ é a transposta da matriz Jacobiana $D\\mathbf{r}(\\boldsymbol\\beta)$, cujas linhas são os gradientes da função vetorial cujos componentes são os resíduos:\n",
    "$$ \\mathbf{r}(\\boldsymbol\\beta) = \\left(y_i - \\varphi(x_i, \\boldsymbol\\beta)\\right)_{i=1, \\ldots, n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interlúdio para o caso linear\n",
    "\n",
    "* Observe que, no caso linear, temos\n",
    "$$\\mathbf{r}(\\boldsymbol\\beta) = A \\boldsymbol\\beta - \\mathbf{b}$$\n",
    "e a equação acima se reduz à equação normal do problema de mínimos quadrados linear:\n",
    "$$ A^t(A\\boldsymbol\\beta) = A^t\\mathbf{b}.\n",
    "$$\n",
    "\n",
    "* Diferentemente do caso linear, onde a hipótese de $A$ ter posto completo (com $m$ colunas linearmente independentes) implica na convexidade de $\\mathbf{r}$ (hessiana positiva-definida), e portanto na unicidade da solução de, o caso não-linear tem a condição acima apenas como **necessária**. Podem existir vários pontos de mínimo local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmos\n",
    "\n",
    "* Os métodos de **Gradiente descendente**, **Gauss-Newton** e **Levenberg-Marquardt**, por exemplo, são algoritmos iterativos clássicos.\n",
    "\n",
    "* Neles, busca-se uma **sequência minimizante** $\\boldsymbol\\beta^{(0)}, \\boldsymbol\\beta^{(1)}, \\ldots$ se aproximando do ponto de mínimo desejado.\n",
    "\n",
    "* A escolha do ponto de partida $\\boldsymbol\\beta^{(0)}$ é delicada, pois pode nos levar à mínimos locais, longe de um bom ajuste.\n",
    "\n",
    "* O algoritmo termina quando algum **critério de parada** é alcançado. Por exemplo:\n",
    "  * quando o erro $E(\\boldsymbol\\beta)$ é suficientemente pequeno;\n",
    "  * quando a variação em $E(\\boldsymbol\\beta)$ é suficientemente pequena;\n",
    "  * quando ${\\boldsymbol\\beta}^{(k+1)}$ está bem próximo de $\\boldsymbol\\beta^{(k)}$; ou\n",
    "  * quando um número máximo de iterações é atingido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradiente descendente\n",
    "\n",
    "* Esse método é bastante intuitivo.\n",
    "\n",
    "* Assumindo que a função $E(\\boldsymbol\\beta)$ seja suave, a sua direção de maior decrescimento é a contrária ao seu gradiente.\n",
    "\n",
    "* A idéia, então, é dar um passo na direção contrária ao gradiente, em cada iteração:\n",
    "\n",
    "    1. Dado um ponto $\\boldsymbol{\\beta}^{(k)}$, em que o erro $E(\\boldsymbol{\\beta}^{(k)})$ ainda não é suficientemente pequeno (caso contrário já teríamos resolvido o problemo), calculamos o gradiente\n",
    "    $$ \\nabla E(\\boldsymbol{\\beta}^{(k)})\n",
    "    $$\n",
    "    1. Dado um \"tamanho de passo\" $\\eta$, \"andamos\" na direção contrária, escolhendo o próximo ponto $\\boldsymbol{\\beta}^{(k+1)}$ como\n",
    "    $$ \\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\eta \\nabla E(\\boldsymbol{\\beta}^{(k)}).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deficiências do método de gradiente conjugado\n",
    "\n",
    "* A sua convergência é relativamente lenta.\n",
    "\n",
    "* A escolha do tamanho de passo $\\eta$ não segue uma receita muito bem definida.\n",
    "\n",
    "* Passos muito grandes podem fazer com que o método não-convirja (\"pule para um lugar mais alto do outro lado do vale\").\n",
    "\n",
    "* Passos muito pequenos podem levar ao acúmulo de erros numéricos (\"pequenas mudanças em um passo curto acarretam em grandes mudanças no ângulo de direcionamento)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gauss-Newton\n",
    "\n",
    "* O método consiste basicamente nos seguintes ingredientes:\n",
    "    1. Pensar no objetivo $E(\\boldsymbol{\\beta}) = \\|\\mathbf{r}\\|^2 = 0$ como um **objetivo para os resíduos**:\n",
    "    $$ \\mathbf{r}(\\boldsymbol{\\beta}) = \\mathbf{0};\n",
    "    $$\n",
    "    1. Dado o $k$-ésimo termo da sequência, obter uma **aproximação afim** da função $\\mathbf{r}$ perto do ponto $\\boldsymbol{\\beta}^{(k)}$;\n",
    "    1. Minimizar o erro quadrático da aproximação afim.\n",
    "    1. Olhar para essa solução como o novo termo $\\boldsymbol{\\beta}^{(k+1)}$ da sequência;\n",
    "    1. Repetir o processo até alcançar um dos critérios de parada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Aproximação afim\n",
    "\n",
    "1. A partir do ponto $\\boldsymbol{\\beta}^{(k)}$, em que o erro $E(\\boldsymbol{\\beta}^{(k)})$ ainda não é suficientemente pequeno (caso contrário já teríamos resolvido o problema), buscamos uma aproximação linear para os resíduos $\\mathbf{r}(\\boldsymbol{\\beta})$.\n",
    "\n",
    "1. Como aproximação linear, é natural tomarmos a linearização em torno do ponto dado:\n",
    "$$ \\mathbf{r}(\\boldsymbol{\\beta}) \\approx \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)}),\n",
    "$$\n",
    "onde $D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})$ é a diferencial de $\\mathbf{r}$, cusas linhas são os gradientes de cada resíduo $r_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Iteração\n",
    "\n",
    "3. Agora, buscamos $\\boldsymbol{\\beta}$ de forma a minimizar o erro quadrático segundo essa aproximação afim\n",
    "$$ \\boldsymbol{\\beta}^{(k+1)} = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\|\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)})\\|^2.\n",
    "$$\n",
    "\n",
    "3. Isso nada mais é do que um problema de mínimos quadrados linear.\n",
    "\n",
    "3. Assumindo-se que as colunas de $D\\mathbf{r}(\\theta^{(k)})$ sejam **linearmente independentes** (i.e. $N\\geq m$ e matriz com posto máximo), a solução é dada pelas equações normais\n",
    "$$ \\left(D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})\\right)\\boldsymbol{\\beta}^{(k+1)} = D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t\\left(D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})\\boldsymbol{\\beta}^{(k)}) - \\mathbf{r}(\\boldsymbol{\\beta}^{(k)})\\right).\n",
    "$$\n",
    "\n",
    "5. Nesse caso, o processo iterativo pode ser escrito na forma\n",
    "$$ \\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\left( D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})\\right)^{-1} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observação:** No caso de uma matriz quadrada ($m=N$) invertível, obtemos o método de Newton $\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^{-1}\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deficiências do Gauss-Newton\n",
    "\n",
    "1. **O método pode divergir**: Isso pode acontecer quando $\\boldsymbol\\beta^{(k+1)}$ não está muito próximo de $\\boldsymbol\\beta^{(k)}$, de modo que o passo seja muito grande;\n",
    "\n",
    "2. **O método pode ter que ser abortado**: A hipótese de que as colunas de $D\\mathbf{r}(\\boldsymbol\\beta^{(k)})$ sejam linearmente independentes pode falhar em alguma iteração, de modo que o cálculo de $\\theta^{(k+1)}$ não seja possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Levenberg-Marquardt\n",
    "\n",
    "* É mais recente, de meados do século XX.\n",
    "\n",
    "* Motivado pelas deficiências do método de Gauss-Newton.\n",
    "\n",
    "* Pode ser visto como um meio termo entre o gradiente descendente e o Gauss-Newton.\n",
    "\n",
    "* É uma forma de um Gauss-Newton amortecido.\n",
    "\n",
    "* Pode ser mais lento, mas é mais robusto do que o Gauss-Newton.\n",
    "\n",
    "* Procura alcançar dois objetivos:\n",
    "    1. Fazer o erro quadrático pequeno.\n",
    "    2. Fazer o passo  $\\|\\boldsymbol\\beta^{(k+1)} - \\boldsymbol\\beta^{(k)}\\|$ também pequeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Implementação dos objetivos\n",
    "\n",
    "* Para \"penalizar\" tanto o erro quando o passo, a ideia é obter $\\boldsymbol\\beta^{(k+1)}$ através da minimização de\n",
    "$$ \\|\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)})\\|^2 + \\lambda\\|\\boldsymbol\\beta - \\boldsymbol\\beta^{(k)}\\|^2.\n",
    "$$\n",
    "\n",
    "* Isso leva em consideração a aproximação afim do erro, sendo, portanto, uma aproximação quadrádica de\n",
    "$$ \\|\\mathbf{r}(\\boldsymbol{\\beta})\\|^2 + \\lambda\\|\\boldsymbol\\beta - \\boldsymbol\\beta^{(k)}\\|^2.\n",
    "$$\n",
    "\n",
    "* O parâmetro $\\lambda$ é um parâmetro positivo de *regularização*, ou *suavização*, ou, ainda, de *amortecimento*.\n",
    "\n",
    "* Esse é um problema de mínimos quadrados linear **multi-objetivo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Iteração\n",
    "\n",
    "* O parâmetro é escolhido a cada passo, $\\lambda=\\lambda^{(k)}$, nos levando ao problema iterativo\n",
    "$$ \\boldsymbol\\beta^{(k+1)} = \\operatorname{argmin}_{\\boldsymbol\\beta}\\left[\\|\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)})\\|^2 + \\lambda^{(k)}\\|\\boldsymbol\\beta - \\boldsymbol\\beta^{(k)}\\|^2\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Resolução\n",
    "\n",
    "* Podemos escrever\n",
    "$$ \\begin{multline*}\n",
    "\\|\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)})\\|^2 + \\lambda\\|\\boldsymbol\\beta - \\boldsymbol\\beta^{(k)}\\|^2 \\\\\n",
    "= \\left\\| \\begin{matrix} \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^{(k)}) \\\\ \\sqrt{\\lambda}(\\boldsymbol\\beta - \\boldsymbol\\beta^{(k)})\\end{matrix}\\right\\|^2 \\\\ \n",
    "= \\left\\| \\left[\\begin{matrix} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) \\\\ \\sqrt{\\lambda}I\\end{matrix}\\right]\\boldsymbol{\\beta}  - \\left(\\begin{matrix} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})\\boldsymbol{\\beta}^{(k)} - \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) \\\\ \\sqrt{\\lambda}\\boldsymbol\\beta^{(k)}\\end{matrix}\\right)\\right\\|^2.\\end{multline*}\n",
    "$$\n",
    "\n",
    "* Por esse ponto de vista, é um clássico problema de mínimos quadrados linear da form $\\operatorname{argmin}\\|A\\boldsymbol\\beta + \\mathbf{y}\\|^2$.\n",
    "\n",
    "* Se $D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})$ tiver posto máximo, assim também o tem a matriz $A$, de forma que o problema tem solução única.\n",
    "\n",
    "* A sua forma normal $A^tA\\boldsymbol{\\beta} = A^t\\mathbf{y}$ nos leva à fórmula de recursão (verifique!)\n",
    "$$\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\left( D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + \\lambda^{(k)}I \\right)^{-1} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Amortecimento\n",
    "\n",
    "* Reescrevemos\n",
    "$$\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\left( D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + \\lambda^{(k)}I \\right)^{-1} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t \\mathbf{r}(\\boldsymbol{\\beta}^{(k)})$$\n",
    "como\n",
    "$$\\left( D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + \\lambda^{(k)}I \\right)\\left(\\boldsymbol{\\beta}^{(k+1)} - \\boldsymbol{\\beta}^{(k)}\\right) = -  D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}).$$\n",
    "\n",
    "* Observe que $\\lambda^{(k)}>0$ tem o papel de reduzir o passo $\\boldsymbol{\\beta}^{(k+1)} - \\boldsymbol{\\beta}^{(k)}$.\n",
    "\n",
    "* Por esse motivo ele é visto como um parâmetro de amortecimento.\n",
    "\n",
    "* Ele também busca evitar abrutos \"buracos\" locais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sobre o parâmetro de amortecimento\n",
    "\n",
    "* O parâmetro $\\lambda^{(k)}$ é também conhecido como parâmetro de confiança, pois outra forma de interpretar o problema de otimização resolvido a cada iteração é como um método de região de confiança (*trust-region method*). \n",
    "\n",
    "* Visto como um parâmetro de regularização/penalização, ele controla qual parte da função objetivo é levada mais em consideração.\n",
    "\n",
    "* Diferentes estratégias para a escolha do parâmetro podem ser seguidas.\n",
    "Maiores informações podem ser encontradas em [Boyd]().   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outros métodos\n",
    "\n",
    "* Há vários outros métodos, alguns sendo variações dos vistos acima (e.g gradiente estocástico), mas não é o nosso objetivo explorar os diversos métodos. Isso fica para um curso de otimização.\n",
    "\n",
    "* Devo mencinar que os métodos como descritos acima são métodos para problemas de minimização *sem restrição*.\n",
    "\n",
    "* Métodos para problemas com restrição podem ser modificados para levar a restrição em consideração ou para atacar um formulação via multiplicadores de Lagrange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Métodos livre de derivada\n",
    "\n",
    "* Devo ressaltar, ainda, os métodos **livres de derivada**, ou *derivative-free*.\n",
    "\n",
    "* São úteis quando não temos a derivada disponível ou ela é muito custosa de se calcular.\n",
    "\n",
    "* Exemplos notáveis são os seguintes.\n",
    "    * **Nelder-Mead:** busca através de polítopos com $m+1$ vértices (e.g. vértices de triângulos no plano). Avalia a função nos vértices de um polítopo, (que muda a cada iteração, exceto pelo ponto \"base\"), retendo o ponto de mínimo dentre os vértices omo um novo ponto base para a formação do próximo polítopo.\n",
    "    * **Algoritmos genéticos:** parte-se de vários pontos escolhidos aleatoriamente e busca-se reduzir o erro em cada ponto através de funções objetivo envolvendo outros critérios.\n",
    "    * **Região de confiança:** busca-se modelos que aproximem bem localmente, aumentando-se aos poucos a região de busca.\n",
    "    * ***Simulated annealing:*** busca-se reduzir uma função \"temperatura\" de maneira probabilística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Verifique a formula do gradiente da função erro $E(\\boldsymbol\\beta) = \\|\\mathbf{r}(\\boldsymbol\\beta)\\|^2$, onde $\\mathbf{r}(\\boldsymbol\\beta)$ é o vetor $\\mathbf{r}(\\boldsymbol\\beta) = \\left(y_i - \\varphi(x_i, \\boldsymbol\\beta)\\right)_{i=1}^N$.\n",
    "\n",
    "1. Obtenha a fórmula\n",
    "$$\\boldsymbol{\\beta}^{(k+1)} = \\boldsymbol{\\beta}^{(k)} - \\left( D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)}) + \\lambda^{(k)}I \\right)^{-1} D\\mathbf{r}(\\boldsymbol{\\beta}^{(k)})^t \\mathbf{r}(\\boldsymbol{\\beta}^{(k)}).$$\n",
    "a partir da forma normal  $A^tA\\boldsymbol{\\beta} = A^t\\mathbf{y}$ como descrito no método de Levenberg-Marquardt.\n",
    "\n",
    "1. Escreva explicitamente a fórmula iterativa $\\beta^{(k+1)} = \\beta^{(k)} + \\ldots$ quando temos apenas um parâmetro $\\beta\\in \\mathbb{R}$ e o modelo tem a forma $y = \\varphi(x, \\beta) = x^2 + \\sin(\\beta x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Referência\n",
    "\n",
    "1. [S. Boyd, L. Vandenberghe, *Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares*, Cambridge University Press, 2018.](https://web.stanford.edu/~boyd/vmls/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--NAVIGATOR-->\n",
    "\n",
    "---\n",
    "[<- 8. Modelos redutíveis ao caso linear nos parâmetros e aplicações](08.00-Modelos_redutiveis_linear_aplicacoes.ipynb) | [Página inicial](00.00-Pagina_inicial.ipynb) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
