{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mínimos quadrados e o ajuste de parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ajuste de parâmetros\n",
    "\n",
    "* Independentemente de como chegamos a um modelo, uma parte importante do processo de modelagem é o ajuste dos parâmetros do modelo.\n",
    "\n",
    "* O tipo de ajuste depende do tipo de modelagem.\n",
    "\n",
    "* Veremos aqui, o método de mínimos quadrados, aplicável em modelos lineares nos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo: Reta passando por dois pontos\n",
    "\n",
    "* Considere, para efeito de motivação, o problema de se achar a reta que passa por dois pontos no plano.\n",
    "\n",
    "* Em uma aplicação, esses pontos seriam os **dados** da **amostra**.\n",
    "\n",
    "* Vamos supor que eles sejam $(2, 1)$ e $(3, 4)$.\n",
    "\n",
    "* Nesse caso, a reta passa por $(2,1)$ e tem inclinação $(4-1)/(3-2) = 3$. Logo, é dada por\n",
    "$$ (y - 1) = 3(x - 2).\n",
    "$$\n",
    "\n",
    "* Ou, de outra forma,\n",
    "$$ y = 3x - 5.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ajuste reta a dois pontos](../../_assets/attachments/img/ajuste_reta_a_dois_pontos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Podemos, também, enxergar isso como um problema de álgebra linear. Isso é feito procurando-se uma reta $y = mx + b$ que passe pelos dois pontos.\n",
    "\n",
    "* Nesse caso, temos o problema de **ajustar os parâmetros** $m$ e $b$ aos **dados** $(2,1)$ e $(3,4)$.\n",
    "\n",
    "* Nesse ajuste, temos as condições\n",
    "$$ \\begin{cases}\n",
    "  b + 2m = 1, \\\\\n",
    "  b + 3m = 4.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Em forma matricial,\n",
    "$$\n",
    "  \\left[ \\begin{matrix} 1 & 2 \\\\ 1 & 3 \\end{matrix}\\right] \\left( \\begin{matrix} m \\\\ b \\end{matrix} \\right) = \\left( \\begin{matrix} 1 \\\\ 4 \\end{matrix} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* É fácil resolver o sistema \n",
    "$$ \\begin{cases}\n",
    "  b + 2m = 1, \\\\\n",
    "  b + 3m = 4.\n",
    "\\end{cases}\n",
    "$$\n",
    "usando uma equação pra escrever $b$ em função de $m$ e sem seguida resolvendo a outra equação para $m$.\n",
    "\n",
    "* Também podemos inverter a matriz associada para achar\n",
    "$$\n",
    "   \\left( \\begin{matrix} b \\\\ m \\end{matrix} \\right) = \\left[ \\begin{matrix} 1 & 2 \\\\ 1 & 3 \\end{matrix}\\right]^{-1} \\left( \\begin{matrix} 1 \\\\ 4 \\end{matrix} \\right) = \\left[ \\begin{matrix} 3 & -2 \\\\ -1 & 1 \\end{matrix}\\right] \\left( \\begin{matrix} 1 \\\\ 4 \\end{matrix} \\right) = \\left( \\begin{matrix} -5 \\\\ 3 \\end{matrix} \\right).\n",
    "$$\n",
    "\n",
    "* Esse problema sempre tem solução, a menos que os dois pontos estejam na vertical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Observações\n",
    "\n",
    "* Na prática, no entanto, o modelo não é perfeito, os dados não são precisos e a solução não é tão simples.\n",
    "\n",
    "* Em alguns casos, podemos ter poucos dados, em outros, podemos ter muitos.\n",
    "\n",
    "* *Poucos dados* nos dão muitas incertezas. Pense no problema de se achar uma reta sobre a qual só temos a informação de um ponto por onde ela passa.\n",
    "\n",
    "* *Muitos dados* nos dão aproximações, nem sempre muito boas. Pense no problema de se achar uma reta que passa perto de três pontos não-colineares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo: Retas passando por um único ponto\n",
    "\n",
    "* Digamos, no entanto, que temos apenas um ponto $(1,2)$. (uma simplificação exagerada do que pode acontecer na prática).\n",
    "\n",
    "* Há infinitas soluções para $m$ e $b$ tais que $y=mx + b$ passe por $(1,2)$, basta que\n",
    "$$ m + b = 2\n",
    "$$\n",
    "\n",
    "* Nesse caso, podemos simplificar o modelo e/ou exigir uma condição a mais.\n",
    "\n",
    "* Podemos simplificar o modelo exigindo que a reta seja horizontal:\n",
    "$$ m = 0 \\text{ e } b = 2 \\quad \\Longrightarrow \\quad \\text{Modelo: } y = 2\n",
    "$$\n",
    "\n",
    "* Ou que ela seja homogênea, i.e. passe pela origem:\n",
    "$$ m = 2 \\text{ e } b = 0 \\quad \\Longrightarrow \\quad \\text{Modelo: } y = x\n",
    "$$\n",
    "\n",
    "* De outro ponto de vista, os modelos acima podem ser vistos como os com o **menor número de parâmetros**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ajuste reta a um pontos](../../_assets/attachments/img/ajuste_reta_a_um_ponto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Podemos, também, exigir que a solução seja a de **menor norma euclidiana**: $\\sqrt{m^2 + b^2}$. Na verdade isso é equivalente a minimizar $m^2 + b^2$, que é mais fácil por não envolver a raiz quadrada. Nesse caso (verifique!),\n",
    "$$ m = 1 \\text{ e } b = 1 \\quad \\Longrightarrow \\quad \\text{Modelo: } y = x + 1\n",
    "$$\n",
    "\n",
    "* As soluções acima são tipos de **regularização** (e.g. [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)),  [Ridge](https://en.wikipedia.org/wiki/Ridge_regression), [Tikhonov](https://en.wikipedia.org/wiki/Tikhonov_regularization)) de um problema **mal-posto** (no caso, com mais de uma solução). A regularização nos dá uma forma de escolher alguma das possíveis soluções de acordo com algum propósito."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo: ajustando uma reta a mais de dois pontos não colineares\n",
    "\n",
    "* Um problema oposto é o de ajustar uma reta a mais de dois pontos.\n",
    "\n",
    "* Isso funciona bem quando todos os pontos são colineares.\n",
    "\n",
    "* Caso contrário, teremos, no máximo, uma *aproximação*.\n",
    "\n",
    "* Como fazer essa aproximação?\n",
    "\n",
    "* A resposta tradicional é a de usar **mínimos quadrados**, ou seja, procurar a reta que *minimiza o erro quadrático*.\n",
    "\n",
    "* Por exemplo, digamos que queiramos ajustar a reta $y=mx + b$ aos pontos $(1,2)$, $(2,1)$ e $(3,4)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ajuste reta a três pontos](../../_assets/attachments/img/ajuste_reta_a_tres_pontos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Se existisse solução exata, ela seria solução do sistema\n",
    "$$ \\begin{cases}\n",
    "  b + m = 2, \\\\\n",
    "  b + 2m = 1, \\\\\n",
    "  b + 3m = 4.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Em forma matricial,\n",
    "$$\n",
    "  \\left[ \\begin{matrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{matrix}\\right] \\left( \\begin{matrix} m \\\\ b \\end{matrix} \\right) = \\left( \\begin{matrix} 2 \\\\ 1 \\\\ 4 \\end{matrix} \\right).\n",
    "$$\n",
    "\n",
    "* É imediato deduzir, a partir da visualização dos pontos no plano, que é impossível achar tal reta. O mesmo pode ser deduzido a partir do sistema linear ou do escalonamento da matriz.\n",
    "\n",
    "* Mas podemos buscar a solução que melhor aproxima os dados no sentido do erro quadrático, como veremos a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mínimos quadrados\n",
    "\n",
    "* A ideia, então, é achar a reta mais próxima dos pontos em algum sentido.\n",
    "\n",
    "* Para cada abscissa $x_i$ de cada ponto $(x_i, y_i)$ da amostra, temos a ordenada correspondente $y_i$ e ordenada $\\hat y_i = mx_i + b$ obtida pelo modelo.\n",
    "\n",
    "* A diferença entre $y_i$ e $\\hat y_i$ é chamada de **resíduo** no ponto $i$:\n",
    "$$ r_i = y_i - \\hat y_i.\n",
    "$$\n",
    "\n",
    "* O **erro** absoluto em cada ponto é\n",
    "$$ |r_i| = |y_i - \\hat y_i|\n",
    "$$\n",
    "\n",
    "* O **erro quadrático** total, ou **soma dos quadrados dos resíduos** *(RSS=residual sum of squares)*, é\n",
    "$$ E = \\sum_{i=1}^N |y_i - \\hat y_i|^2,\n",
    "$$\n",
    "onde $N=3$, nesse caso (três pontos).\n",
    "\n",
    "* Buscamos, então, minimizar esse erro:\n",
    "$$ \\operatorname{argmin}_{m,b\\in \\mathbb{R}} \\sum_{i=1}^N |y_i - \\hat y_i|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* A função a ser minimizada é suave (quadrática), convexa e coerciva:\n",
    "$$  E(m,b) = \\sum_{i=1}^N |y_i - \\hat y_i|^2 = \\sum_{i=1}^N |y_i - mx_i - b|^2\n",
    "$$\n",
    "\n",
    "* Portanto, existe pelo menos uma solução.\n",
    "\n",
    "* As soluções podem ser encontradas procurando-se o ponto crítico da função\n",
    "$$ \\nabla E(m,b) = (0,0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* O gradiente da função $E=\\sum_i |y_i - mx_i - b|^2$ é o vetor $\\nabla E(m,b) = (\\partial_m E, \\partial_b E)$ das derivadas parciais\n",
    "$$ \\frac{\\partial E}{\\partial m} = -2\\sum_i (y_i - mx_i - b)x_i = 2(\\sum_i x_i^2)m  + 2(\\sum_i xi)b -2\\sum_i x_iy_i,\n",
    "$$\n",
    "$$ \\frac{\\partial E}{\\partial b} = -2\\sum_i (y_i - mx_i - b) = 2(\\sum_i x_i)m + 2(\\sum_i 1))b - 2\\sum_i y_i.\n",
    "$$\n",
    "\n",
    "* Portanto, os pontos críticos são dados pelo sistema, escrito em forma matricial,\n",
    "$$\n",
    " \\left[ \\begin{matrix} \\sum_i x_i & \\sum_i x_i^2 \\\\ \\sum_i 1 & \\sum_i x_i\\end{matrix} \\right] \\left(\\begin{matrix} b \\\\ m \\end{matrix}\\right) = \\left(\\begin{matrix} \\sum_i x_iy_i \\\\ \\sum_i y_i \\end{matrix}\\right).\n",
    "$$\n",
    "\n",
    "* Esse sistema tem uma certa estrutura, que pode não ser clara de imediato, mas que tem a seguinte forma\n",
    "$$\n",
    " \\left[ \\begin{matrix} 1 & \\ldots & 1 \\\\ x_1 & \\ldots & x_N \\end{matrix} \\right] \\left[ \\begin{matrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{matrix} \\right] \\left(\\begin{matrix} b \\\\ m \\end{matrix}\\right) = \\left[ \\begin{matrix} 1 & \\ldots & 1 \\\\ x_1 & \\ldots & x_N \\end{matrix} \\right] \\left(\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Mais ainda, note que a matriz à esquerda é a transposta da matriz seguinte, i.e.\n",
    "$$\\left[ \\begin{matrix} 1 & \\ldots & 1 \\\\ x_1 & \\ldots & x_N \\end{matrix} \\right] = \\left[ \\begin{matrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{matrix} \\right]^t\n",
    "$$\n",
    "\n",
    "* Isso pode ser visto de uma maneira mais natural, a partir da formulação\n",
    "$$ \\operatorname{argmin}_{m,b\\in \\mathbb{R}} \\sum_{i=1}^N |y_i - mx_i - b|^2.\n",
    "$$\n",
    "\n",
    "* A função a ser minimizada por ser reescrita como\n",
    "$$  \\left\\|\\left(\\begin{matrix} y_1 - b - mx_1\\\\ \\vdots \\\\ y_N - b - m x_N\\end{matrix}\\right)\\right\\|^2 = \\left\\|\\left(\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right) - b \\left(\\begin{matrix} 1 \\\\ \\vdots \\\\ 1 \\end{matrix}\\right) - m \\left(\\begin{matrix} x_1 \\\\ \\vdots \\\\ x_N \\end{matrix}\\right)\\right\\|^2 = \\left\\|\\left(\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right) - \\left[\\begin{matrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{matrix}\\right]\\left(\\begin{matrix} b \\\\ m \\end{matrix}\\right)\\right\\|^2.\n",
    "$$\n",
    "\n",
    "* Ou seja, $\\|\\mathbf{y} - A \\boldsymbol{\\beta}\\|^2$, onde\n",
    "$$ A = \\left[\\begin{matrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{matrix}\\right], \\quad \\boldsymbol{\\beta} = \\left(\\begin{matrix} b \\\\ m \\end{matrix}\\right), \\quad \\mathbf{y} = \\left(\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Para minimizar $E(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - A\\boldsymbol{\\beta}\\|^2$, olhamos, novamente, para os seus pontos críticos.\n",
    "\n",
    "* A derivada direcional de $E(\\boldsymbol{\\beta})$ na direção de um vetor unitário $\\boldsymbol{\\alpha}$ é (verifique!)\n",
    "$$ \\nabla E(\\boldsymbol{\\beta})\\cdot \\boldsymbol{\\alpha} = -2(\\mathbf{y} - A\\boldsymbol{\\beta})\\cdot (A\\boldsymbol{\\alpha}).\n",
    "$$\n",
    "\n",
    "* Podemos reescrever isso usando exatamente a transposta de $A$:\n",
    "$$ \\nabla E(\\boldsymbol{\\beta})\\cdot \\boldsymbol{\\alpha} = - 2(A^t(\\mathbf{y} - A\\boldsymbol{\\beta}))\\cdot \\boldsymbol{\\alpha}.\n",
    "$$\n",
    "\n",
    "* Ou seja, \n",
    "$$ \\nabla E(\\boldsymbol{\\beta}) = - 2A^t(\\mathbf{y} - A\\boldsymbol{\\beta}).\n",
    "$$\n",
    "\n",
    "* No ponto crítico, devemos ter $\\nabla E(\\boldsymbol{\\beta}) = \\mathbf{0}$, ou seja\n",
    "$$ A^tA\\boldsymbol{\\beta} = A^t\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "* Note que isso é exatamente a fórmula obtida anteriormente. Mas agora está mais fácil de ser generalizada!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ajustando polinômios de ordem mais alta\n",
    "\n",
    "* Não precisamos nos restringir a modelos lineares $y=mx + b$.\n",
    "\n",
    "* Podemos procurar polinômios de ordem mais alta\n",
    "$$ y = \\beta_0 + \\beta_1x + \\beta_2 x^2 + \\ldots + \\beta_m x^m.\n",
    "$$\n",
    "\n",
    "* E com um determinado número de dados $(x_1, y_1), \\ldots, (x_N, y_N)$.\n",
    "\n",
    "* Nesse caso, os resíduos são\n",
    "$$  y_i - \\beta_0 - \\beta_1x_i - \\beta_2 x_i^2 - \\ldots - \\beta_m x_i^m.\n",
    "$$\n",
    "\n",
    "* O erro quadrático tem a forma matricial\n",
    "$$ \\left\\|\\left(\\begin{matrix} y_1 \\\\ \\vdots \\\\ y_N \\end{matrix}\\right) - \\left[\\begin{matrix} 1 & x_1 & \\ldots & x_1^m \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_N & \\ldots & x_N^m \\end{matrix}\\right]\\left(\\begin{matrix} \\beta_0 \\\\ \\ldots \\\\ \\beta_m \\end{matrix}\\right)\\right\\|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Os parâmetros a serem ajustados são\n",
    "$$ \\boldsymbol{\\beta} = \\left( \\begin{matrix} \\beta_0 \\\\ \\vdots \\\\ \\beta_m \\end{matrix} \\right).\n",
    "$$\n",
    "\n",
    "* E a matriz associada tem a forma de uma **matriz de Vandermonde**:\n",
    "$$ A = \\left[\\begin{matrix} 1 & x_1 & \\ldots & x_1^m \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_N & \\ldots & x_N^m \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "* As soluções $\\boldsymbol{\\beta}$ são dadas pela chamada **forma normal**\n",
    "$$ A^tA\\boldsymbol{\\beta} = A^t\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Note que\n",
    "$$ A^tA = \\left[\\begin{matrix} 1 & 1 & \\ldots & 1 \\\\ x_1 & x_2 & \\ldots & x_N \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ x_1^m & x_N^m & \\ldots & x_N^m \\end{matrix}\\right] \\left[\\begin{matrix} 1 & x_1 & \\ldots & x_1^m \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_N & \\ldots & x_N^m \\end{matrix}\\right]\n",
    "$$\n",
    "\n",
    "* Note que $A^tA$ é uma matriz quadrada $m\\times m$, para exatamente $m$ parâmetros.\n",
    "\n",
    "* A sua invertibilidade depende do posto de $A$.\n",
    "\n",
    "* Caso seja invertível, a solução é única e pode ser escrita na forma\n",
    "$$ \\boldsymbol{\\beta} = (A^tA)^{-1}A^t\\mathbf{y}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Invertibilidade da matriz $A^tA$\n",
    "\n",
    "* A invertibilidade de $A^tA$ está diretamente ligada ao posto de $A$.\n",
    "\n",
    "* Observe que o núcleo de $A^t$ é ortogonal à imagem de $A$. De fato, \n",
    "$$ \\text{se} \\quad A^t\\boldsymbol{\\alpha} = 0 \\quad \\text{e} \\quad \\boldsymbol{\\gamma} = A\\boldsymbol{\\beta},\n",
    "$$\n",
    "então\n",
    "$$ \\boldsymbol{\\alpha} \\cdot \\boldsymbol{\\gamma} = \\boldsymbol{\\alpha} \\cdot A\\boldsymbol{\\beta} = A^t\\boldsymbol{\\alpha} \\cdot \\boldsymbol{\\beta} = \\mathbf{0} \\cdot \\boldsymbol{\\beta} = \\mathbf{0}.\n",
    "$$\n",
    "* Assim, se $A^tA\\boldsymbol{\\beta} = 0$: \n",
    "  * $A\\boldsymbol{\\beta}$ está no núcleo de $A^t$;\n",
    "  * Então $A\\boldsymbol{\\beta}$ é ortogonal à imagem de $A$;\n",
    "  * Mas $A\\boldsymbol{\\beta}$ também está na imagem de $A$;\n",
    "  * Assim, $A\\boldsymbol{\\beta}$ tanto é ortogonal como pertence à imagem de $A$.\n",
    "  * Logo, $A\\boldsymbol{\\beta} = 0$.\n",
    "* E obviamente, se $A\\boldsymbol{\\beta} = 0$, então $A^tA\\boldsymbol{\\beta} = 0$. Portanto, temos a equivalência\n",
    "$$ \\text{Núcleo de } A^tA = \\text{Núcleo de } A.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Lembremos, nesse momento, que uma matriz quadrada é invertível se e somente se o seu núcleo se reduz à origem.\n",
    "\n",
    "* Assim, $A^tA$ é invertível se, e somente se, o núcleo de $A$ tem dimensão nula. Isso é equivalente a dizer que o seu posto é máximo.\n",
    "\n",
    "* Finalmente, $A$ tem posto máximo/dimensão nula se pelo menos $m$ colunas são linearmente independentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Como podemos garantir que $A$ tenha posto máximo? Ou de outra forma, que tenha $m$ colunas linearmente independentes?\n",
    "\n",
    "* Para isso, é, primeiramente, necessário que o número de pontos da amostra seja maior do que o número de parâmetros: $N\\geq m$.\n",
    "\n",
    "* Caso contrário, como o contradomínio de $A$ é $N$, o seu posto é limitado por $N$:\n",
    "$$\\operatorname{posto} A \\leq N<m.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Em segundo lugar, quando $N \\geq m$, precisamos que as amostras sejam obtidas em pelo menos $m$ abscissas distintas $x_i$.\n",
    "\n",
    "* De fato, com $m$ abscissas distintas, podemos reordenar as linhas de $A$ para que as abscissas distintas sejam $x_1, \\dots, x_m$. \n",
    "\n",
    "* Em seguida, olhamos para as primeiras $m$ linhas da matriz $A$, que vamos chamar de $A_m$:\n",
    "$$ A = \\left[\\begin{matrix} 1 & x_1 & \\ldots & x_1^m \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ 1 & x_m & \\ldots & x_m^m \\end{matrix}\\right].\n",
    "$$\n",
    "\n",
    "* $A_m$ é uma matriz quadrada $m\\times m$.\n",
    "\n",
    "* $A_m$ nada mais é do que composição de $A$ com a projeção $P_m$ de $\\mathbb{R}^N= \\mathbb{R}^m \\times \\mathbb{R}^{N-m}$ nas $m$ primeiras coordenadas $\\mathbb{R}^m$.\n",
    "\n",
    "* Assim, se $A_m$ for invertível, então $A_m=P_mA$ tem posto $m$, logo $A$ também o tem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $A_m$ é uma matriz de Vandermonde e o seu determinante é dado por\n",
    "$$ \\det A_m = \\prod_{j>i} (x_j-x_i).\n",
    "$$\n",
    "\n",
    "* Há várias demontrações disso, veja em [Vandermonde matrix](https://en.wikipedia.org/wiki/Vandermonde_matrix), mas a mais básica, feita por indução na dimensão $m$, não está lá.\n",
    "\n",
    "* De qualquer forma, observe que esse determinante é não-nulo se, e somente se, todos os $x_i$'s, para $i=1, \\ldots, m$, são distintos.\n",
    "\n",
    "* Isso é exatamente a nossa hipótese de que pelo menos $m$ dados são obtidos em abscissas distantes.\n",
    "\n",
    "* Assim, sendo o determinante não-nulo, a matriz $A_m$ é invertível e $A$ tem posto $m$, como queríamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outros modelos\n",
    "\n",
    "* O modelo não precisa ser um polinômio, nem ser de um única variável, para ser tratado como feito acima.\n",
    "\n",
    "* Pode ser, por exemplo, qualquer função da forma\n",
    "$$ y = \\beta_0 f_0(x) + \\beta_1 f_1(x) + \\ldots + \\beta_m f_m(x)\n",
    "$$\n",
    "\n",
    "* Nesse caso, \n",
    "$$ A = \\left[\\begin{matrix} f_0(x_1) & f_1(x_1) & \\ldots & f_m(x_1) \\\\ \\vdots & \\vdots & \\cdots & \\vdots \\\\ f_0(x_N) & f_1(x_N) & \\ldots & f_m(x_N) \\end{matrix}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Ou, em duas variáveis,\n",
    "$$ y = \\beta_0 + \\beta_{1,0}x_1 + \\beta_{0,1}x_2 + \\beta_{1,1}x_1x_2 + \\ldots + \\beta_{m_1, m_2}x_1^{m_1}x_2^{m_2} \n",
    "$$\n",
    "\n",
    "* Ou, em várias variáveis\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_k x_k.\n",
    "$$\n",
    "\n",
    "* Ou, analogamente, em mais de duas variáveis e com termos não necessariamente polinomiais.\n",
    "\n",
    "* O que é importante é que o modelo seja linear nos parâmetros $\\boldsymbol{\\beta}$. Assim, a forma da matriz $A$ pode mudar, mas o problema de mínimos quadrados continua sendo resolvido por\n",
    "$$ A^tA\\boldsymbol{\\beta} = A^t\\mathbf{y}.\n",
    "$$\n",
    "\n",
    "* A questão da invertibilidade de $A^tA$ vai depender, analogamente, da separação dos valores correspondentes dos termos associados aos parâmetros. (Pense isso!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Mostre que $(m,b) = (1,1)$ é a solução de $m + b = 2$ que minimiza $m^2 + b^2$.\n",
    "\n",
    "1. Mostre que $(m,b) = (2,0)$ é a solução de $m + b = 2$ que minimiza $\\epsilon|m| + |b|$ para $0<\\epsilon<1$. O que acontece no caso $\\epsilon=1$? E quando $\\epsilon>1$? E se objetivo for minimizar $\\max\\{|m|,|b|\\}$? Ou minimizar $\\operatorname{sgn}|m|+\\operatorname{sgn}|b|$ (onde $\\operatorname{sign}(r) = 0$, se $r=0$, ou $r/|r|$, se $r\\neq 0$)?\n",
    "\n",
    "1. Em qual das abscissas $x=1, \\ldots, 5$ temos o resíduo com o maior erro no caso em que o modelo é $\\hat y= 5x + x^2$ e a amostra é dada pelos pontos $(x,y) = (1, 2)$, $(2, 3)$, $(3, 4)$, $(4, 3)$ e $(5, 2)$?\n",
    "\n",
    "1. O que acontece se quisermos ajustar uma parábola $y=ax + bx^2 + c$ aos dados $(1,1)$, $(1,3)$ e $(2,2)$?\n",
    "\n",
    "1. Se quisermos ajustar um modelo $y=\\beta_0 + \\beta_1\\sin(x) + \\beta_2\\cos(x)$ a dados $(x_1,y_1)$, $(x_2,y_2)$, $(x_3,y_3)$, qual a condição em $x_1, x_2, x_3$ que garante que existe um, e somente um, conjunto de parâmetros $\\boldsymbol{\\beta}=(\\beta_0, \\beta_1, \\beta_2)$ que melhor ajusta o modelo no sentido dos mínimos quadrados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<!--NAVIGATOR-->\n",
    "\n",
    "---\n",
    "[<- 4. Ajuste de parâmetros](04.00-Ajuste_parametros.ipynb) | [Página inicial](00.00-Pagina_inicial.ipynb) | [4.2. Modelos redutíveis ao caso linear nos parâmetros e aplicações ->](04.02-Modelos_redutiveis_linear_aplicacoes.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
