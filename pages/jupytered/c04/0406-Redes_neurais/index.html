<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="" />
  <meta name="author" content="and contributors" />
   <title>Redes neurais</title>  
  <link rel="shortcut icon" type="image/png" href="/modelagem_matematica/assets/images/favicon.png"/>
  <link rel="stylesheet" href="/modelagem_matematica/css/base.css"/>
  
  <script src="/modelagem_matematica/libs/mousetrap/mousetrap.min.js"></script>

  
    <link rel="stylesheet" href="/modelagem_matematica/libs/highlight/github.min.css">
    <script src="/modelagem_matematica/libs/highlight/highlight.pack.js"></script>
    <script src="/modelagem_matematica/libs/highlight/julia.min.js"></script>
    <script>
      document.addEventListener('DOMContentLoaded', (event) => {
        document.querySelectorAll('pre').forEach((el) => {
          hljs.highlightElement(el);
        });
      });
    </script>
  

  
    <link rel="stylesheet" href="/modelagem_matematica/libs/katex/katex.min.css">
  
</head>

<body>

  <div class="books-container">

  <aside class="books-menu">
  <input type="checkbox" id="menu">
  <label for="menu">☰</label>

  <div class="books-title">
    <a href="/modelagem_matematica/">Modelagem Matemática</a>
  </div>

  <br />

  <div class="books-subtitle">
    Notas de aula
  </div>

  <br />

  <div class="books-author">
    <a href="https://rmsrosa.github.io">Ricardo M. S. Rosa</a>
  </div>

  <div class="books-menu-content">
    <div class="menu-level-1">
    <li><a href="/modelagem_matematica/pages/intro">Introdução</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE I</li>
    </div>
    <div class="menu-level-1">
    <li>1. Preliminares</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c01/0101-Aspectos_curso">1.1. Aspectos do curso</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c01/0102-Instalando_acessando_Julia">1.2. Instalando e acessando o Julia</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c01/0103-Primeiros_passos_Julia">1.3. Primeiros passos em Julia</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE II</li>
    </div>
    <div class="menu-level-1">
    <li>2. Princípios de Modelagem Matemática</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c02/0201-Principios_basicos">2.1. Princípios básicos de modelagem</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c02/0202-Exemplos_tipos_modelagem">2.2. Exemplos de tipos de modelagem</a></li>
    </div>
    <div class="menu-level-1">
    <li>3. Análise Dimensional</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c03/0301-Quantidades_unidades_dimensoes">3.1. Quantidades, unidades e dimensões</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c03/0302-BuckinghamPi">3.2. Análise dimensional e o Teorema de Buckingham-Pi</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c03/0303-Unidades_Julia">3.3. Trabalhando com unidades e dimensões em Julia</a></li>
    </div>
    <div class="menu-level-1">
    <li>4. Ajuste de Parâmetros</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0401-Minimos_quadrados_ajuste">4.1. Mínimos quadrados e o ajuste de parâmetros</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0402-Exemplos_ajuste_linear">4.2. Exemplos de ajuste linear</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0403-Modelos_redutiveis_linear_aplicacoes">4.3. Modelos redutíveis ao caso linear nos parâmetros e aplicações</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0404-Minimos_quadrados_nao_linear">4.4. Mínimos quadrados não-linear</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0405-Exemplos_ajuste_naolinear">4.5. Exemplos de ajuste não-linear de parâmetros</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0406-Redes_neurais">4.6. Redes neurais</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c04/0407-Ajuste_em_redes_neurais">4.7. Ajuste de parâmetros em modelos de redes neurais</a></li>
    </div>
    <div class="menu-level-1">
    <li>5. Erros e Incertezas</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c05/0501-Erros_e_incertezas">5.1. Erros e incertezas</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c05/0502-Minimos_quadrados_verossimilhanca">5.2. Mínimos quadrados, maximização da verossimilhança e quantificação de incertezas em regressões lineares</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c05/0503-Propagacao_incertezas">5.3. Propagação de incertezas</a></li>
    </div>
    <div class="menu-level-1">
    <li>6. Avaliação de Modelos</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c06/0601-Qualidade_do_modelo">6.1. Qualidade do ajuste</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c06/0602-Validacao_do_modelo">6.2. Validação de modelos</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c06/0603-Comparacao_de_modelos">6.3. Comparação de modelos</a></li>
    </div>
    <div class="menu-level-1">
    <li>PARTE III</li>
    </div>
    <div class="menu-level-1">
    <li>7. Mecânica</li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0701-Mecanica_Newtoniana">7.1. Mecânica Newtoniana</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0702-Mecanica_Lagrangiana">7.2. Mecânica Lagrangiana</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0703-Conservacao_contexto_Newtoniano">7.3. Leis de conservação em um contexto Newtoniano</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0704-Conservacao_contexto_Lagrangiano">7.4. Leis de conservação em um contexto Lagrangiano</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0705-Hamiltonianos">7.5. Hamiltonianos</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0706-Pendulo">7.6. Análise do período de um pêndulo planar simples</a></li>
    </div>
    <div class="menu-level-2">
    <li><a href="/modelagem_matematica/pages/jupytered/c07/0707-Pendulo_angulos_grandes">7.7. Experimentos com pêndulos</a></li>
    </div>
<div>


  
    <a href="https://github.com/rmsrosa/modelagem_matematica/tree/modmat2022p1"><img src="/modelagem_matematica/assets/images/GitHub-Mark-32px.png" alt="GitHub repo" width="18" style="margin:5px 5px" align="left"></a>

  

</aside>


  <div class="books-content">

    
      <div class="navbar">
    <p id="nav">
<span id="nav-prev" style="float: left;">
<a class="menu-level-1" href="/modelagem_matematica/pages/jupytered/c04/0405-Exemplos_ajuste_naolinear">4.5. Exemplos de ajuste não-linear de parâmetros <kbd>←</kbd></a>
</span>
<span id="nav-next" style="float: right;">
    <a class="menu-level-1" href="/modelagem_matematica/pages/jupytered/c04/0407-Ajuste_em_redes_neurais"><kbd>→</kbd> 4.7. Ajuste de parâmetros em modelos de redes neurais</a>
</span>
    </p>
</div>
</br></br>

    

    
      <div class="badges">
<p>
<a href="https://nbviewer.org/urls/rmsrosa.github.io/modelagem_matematica/generated/jupytered/c04/0406-Redes_neurais.ipynb"><img align="left" src="https://img.shields.io/badge/view%20in-nbviewer-orange" alt="View in NBViewer" title="View Jupyter notebook in NBViewer"></a>
<a href="https://mybinder.org/v2/gh/rmsrosa/modelagem_matematica/julia-env-for-binder-2022p1?urlpath=git-pull%3Frepo%3Dhttps://github.com/rmsrosa/modelagem_matematica%26urlpath%3Dlab/tree%252Fmodelagem_matematica/generated/jupytered/c04/0406-Redes_neurais.ipynb%26branch%3Dgh-pages"><img align="left" src="https://mybinder.org/badge.svg" alt="Open in binder" title="Open in binder"></a>
<a href="/modelagem_matematica/generated/jupytered/c04/0406-Redes_neurais.ipynb"><img align="left" src="https://img.shields.io/badge/download-notebook-blue" alt="Download notebook" title="Download Jupyter notebook"></a>
<a href="/modelagem_matematica/src/jupyter/c04/0406-Redes_neurais.ipynb"><img align="left" src="https://img.shields.io/badge/view-source-lightblue" alt="View source" title="View source"></a>
</p>
</div></br>

    
<h1 id="get_title"><a href="#get_title" class="header-anchor">4.6. Redes neurais</a></h1>
<ul>
<li><p>Grosso modo, redes neurais são funções \(f:\mathbb{R}^n \rightarrow \mathbb{R}^m\) com um determinado tipo de estrutura.</p>
</li>
<li><p>Nesse ponto, não são diferentes de polinômios, cuja estrutura é a de combinação linear de monômios: \(f(x) = a_0 + a_1x + \ldots + a_k x^k\). </p>
</li>
<li><p>Ou de polinômios senoidais, cuja estrutura é a de combinação linear de senos e/ou cossenos com frequências e fases variadas, como na série de Fourier: \(f(t) = a_1\sin(\theta_1 + \omega_1 t) + \ldots + a_k\sin(\theta_k + \omega_k t)\).</p>
</li>
<li><p>E de muitas outras classes de funções aproximantes &#40;polinômios de Chebyshev, polinômios de Lagrange, aproximações de Padé por funções racionais, etc.&#41;</p>
</li>
<li><p>Uma das redes neurais mais simples que podemos considerar é a rede <em>pró-alimentada</em> de camadas <em>densas</em>, formada pela composição de funções da forma</p>
</li>
</ul>
\[
x \mapsto g(Wx + b),
\]
<p>onde \(g : \mathbb{R} \rightarrow \mathbb{R}\) é chamada de <strong>função de ativação</strong>, \(W\) é uma matriz de <strong>pesos</strong> e \(b\), de <strong>viés</strong>.</p>
<ul>
<li><p>Podemos ter \(x\), \(W\) e \(b\) escalares, mas, em geral, \(x\) e \(b\) são vetores e \(W\), uma matriz &#40;ou tensores de maior dimensão&#41;. Nesse caso, \(g\) age em cada elemento do vetor \(Wx + b\), gerando um vetor de mesma dimensão.</p>
</li>
<li><p>Há vários pacotes Julia para faciliar a construção e o treinamento de redes neurais. O mais conhecido e utilizado deles é o <a href="https://github.com/FluxML/Flux.jl">Flux.jl</a>.</p>
</li>
</ul>
<pre><code class="language-julia">using LinearAlgebra: ⋅
using Flux
using Plots
using ChainPlots</code></pre>
<h2 id="perceptron_com_dois_sinais_de_entrada"><a href="#perceptron_com_dois_sinais_de_entrada" class="header-anchor">Perceptron com dois sinais de entrada</a></h2>
<ul>
<li><p>Vamos começar com um <strong>percetron</strong>, o <em>bloco-construtor</em> da rede neural de perceptrons originalmente proposta por <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a>, nos anos 1950-1960.</p>
</li>
<li><p>Pra começar, um <strong>perceptron de duas entradas</strong> é uma função que recebe dois sinais &#40;valores&#41; \(x_1\) e \(x_2\) e que, de acordo com <strong>pesos</strong> \(w_1, w_2\) e de um <strong>limiar</strong> \(r\), retorna um valor \(0\) ou \(1\).</p>
</li>
<li><p>A regra associada a esses parâmetros é</p>
</li>
</ul>
\[
  \text{saída} = 
    \begin{cases}
      0, & \displaystyle \text{se } w_1x_1 + w_2x_2 \leq r, \\
      1, & \displaystyle \text{se }  w_1x_1 + w_2x_2 > r.
    \end{cases}
\]
<ul>
<li><p>Ou seja, o <em>neurônio</em> é <strong>ativado</strong> se o sinal ponderado \(w_1x_1 + w_2x_2\) está <strong>acima do limiar</strong>, caso contrário, ele não é ativado.</p>
</li>
</ul>
<h2 id="perceptrons_com_múltiplos_sinais_de_entrada"><a href="#perceptrons_com_múltiplos_sinais_de_entrada" class="header-anchor">Perceptrons com múltiplos sinais de entrada</a></h2>
<ul>
<li><p>Isso pode ser generalizado para um número arbitrário de sinais de entrada \(x_1,\ldots,x_n\), com pesos \(w_1, \ldots, w_n\).</p>
</li>
<li><p>E, atualmente, se usa \(b=-r\), denominado <strong>viés</strong>, de forma que o sinal de saída fica sendo</p>
</li>
</ul>
\[
  \text{saída} = 
    \begin{cases}
      0, & \displaystyle \text{se } \sum_{i=1}^n w_ix_i + b \leq 0, \\
      1, & \displaystyle \text{se }  \sum_{i=1}^n w_ix_i + b > 0.
    \end{cases}
\]
<ul>
<li><p>Isso pode ser escrito de uma forma mais compacta com uma <strong>função de ativação</strong></p>
</li>
</ul>
\[ h(s) = \max\{0,\operatorname{sgn}(x)\} = \begin{cases} 1, & s>0 \\ 0, & s\leq 0 \end{cases}
\]
<ul>
<li><p>E com um <strong>matriz de pesos</strong></p>
</li>
</ul>
\[
W = \left[w_1 w_2 \ldots w_n\right] \in \mathbb{R}^{1 \times n}.
\]
<ul>
<li><p>Assim,</p>
</li>
</ul>
\[ \text{saída} = h\left(w_1x_1 + \ldots + w_nx_n + b\right) = h\left(Wx + b\right).
\]
<h2 id="implementando_um_perceptron_em_julia"><a href="#implementando_um_perceptron_em_julia" class="header-anchor">Implementando um perceptron em Julia</a></h2>
<ul>
<li><p>Um perceptron pode ser facilmente implementado em <code>julia</code>:</p>
</li>
</ul>
<pre><code class="language-julia">n &#61; 2                      # número de entradas
W &#61; &#91;0.6 0.8&#93;              # pesos
b &#61; 1.0                    # viés
h&#40;s&#41; &#61; ifelse&#40;s &gt; 0.0, 1.0, 0.0&#41;   # função de ativação

l&#40;x, h, W, b&#41; &#61; h.&#40;W * x .&#43; b&#41;         # perceptron</code></pre>
<pre><code class="language-julia">l &#40;generic function with 1 method&#41;</code></pre>
<ul>
<li><p>Podemos aplicar <code>l</code> a escalares ou vetores e/ou matrizes.</p>
</li>
</ul>
<pre><code class="language-julia">l&#40;2, h, 0.6, 1&#41;</code></pre>
<pre><code class="language-julia">1.0</code></pre>
<pre><code class="language-julia">l&#40;&#91;1, 2&#93;, h, &#91;0.6 0.2&#93;, &#91;2, 2&#93;&#41;</code></pre>
<pre><code class="language-julia">2-element Vector&#123;Float64&#125;:
 1.0
 1.0</code></pre>
<pre><code class="language-julia">l&#40;&#91;1 2; 3 4&#93;, h, &#91;-1 1&#93;, &#91;1; 2&#93;&#41;</code></pre>
<pre><code class="language-julia">2×2 Matrix&#123;Float64&#125;:
 1.0  1.0
 1.0  1.0</code></pre>
<ul>
<li><p>A função de ativação tem a forma de um &quot;degrau&quot;:</p>
</li>
</ul>
<pre><code class="language-julia">plot&#40;-2:0.001:2, h, legend&#61;false,
    title&#61;&quot;Gráfico da função de ativação s ↦ ifelse&#40;s&gt;0, 1, 0&#41;&quot;, titlefont&#61;11&#41;</code></pre>
<img src="/modelagem_matematica/assets/pages/jupytered/c04/0406-Redes_neurais/code/images/0406-Redes_neurais_6_1.png" alt="">
<ul>
<li><p>Com o peso ponderado, temos um comportamento &quot;degrau&quot; análogo, só que multi-dimensional:</p>
</li>
</ul>
<pre><code class="language-julia">surface&#40;-5:0.1:5, -5:0.1:5, &#40;x,y&#41; -&gt; l&#40;&#91;x,y&#93;, h, W, b&#41;&#91;1&#93;, c&#61;:bluesreds,
    title&#61;&quot;Gráfico do sinal de saída de um perceptron com duas entradas&quot;,
    titlefont&#61;11&#41;</code></pre>
<img src="/modelagem_matematica/assets/pages/jupytered/c04/0406-Redes_neurais/code/images/0406-Redes_neurais_7_1.png" alt="">
<h2 id="redes_de_perceptrons"><a href="#redes_de_perceptrons" class="header-anchor">Redes de perceptrons.</a></h2>
<ul>
<li><p>A função de ativação do perceptron é perfeita para representar circuitos digitais.</p>
</li>
<li><p>De fato, redes de perceptrons podem ser estruturadas para fazer qualquer operação booleana &#40;AND, NOT, OR, XOR, NAND, etc...&#41;</p>
</li>
<li><p>E junto com isso, qualquer operação aritmética binária &#40;adição, subtração, multiplicação, divisão&#41;.</p>
</li>
<li><p>Redes de perceptrons são obtidas encadeando-se diversos perceptrons como acima, com uma ou mais entradas.</p>
</li>
<li><p>Mas isso deve ser construído explicitamente.</p>
</li>
<li><p>Redes de perceptrons não são boas de se <em>treinar</em> &#40;i.e. ajustar os parâmetros \(W\) e \(b\), ou outros em redes mais complexas&#41;</p>
</li>
<li><p>De fato, sendo a função de ativação constante por partes, o seu gradiente é nulo quase sempre, então não podemos utilizar métodos clássicos de otimização. Métodos livres de derivada funcionam, mas perde-se muito em eficiência, principalmente ao se treinar redes com dezenas, centenas, milhares, bilhões ou trilhões de parâmetros.</p>
</li>
<li><p>Vale mencionar que o <a href="https://en.wikipedia.org/wiki/Deep_Blue_&#40;chess_computer&#41;">Deep Blue</a>, um sistema para jogar xadrez desenvolvido há quase trinta anos pela <a href="https://en.wikipedia.org/wiki/IBM">IBM</a> e que venceu o grande campeão mundial <a href="https://en.wikipedia.org/wiki/Garry_Kasparov">Garry Kasparov</a>, possuia mais de oito mil parâmetros.</p>
</li>
<li><p>Atualmente, há redes neurais com trilhões de parâmetros &#40;e.g. <a href="https://www.hyro.ai/post/mythbusting-googles-new-trillion-parameter-ai-language-model">Mythbusting Google’s New Trillion-Parameter AI Language Model</a>&#41;.</p>
</li>
<li><p>Para resolver isso, precisamos de funções de ativação suaves.</p>
</li>
</ul>
<h2 id="outras_funções_de_ativação"><a href="#outras_funções_de_ativação" class="header-anchor">Outras funções de ativação</a></h2>
<ul>
<li><p>Há várias outras funções de ativação comumente utilizadas, com as características de</p>
<ul>
<li><p>ser suave;</p>
</li>
<li><p>variar entre um sinal de saída &quot;baixo&quot; e um &quot;alto&quot;.&quot;</p>
</li>
</ul>
</li>
<li><p>Podemos definir essas funções explicitamente, ou pegar emprestado diretamente do <code>Flux.jl</code>.</p>
</li>
<li><p>Aqui a lista de <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#Activation-Functions-1">funções de ativação</a> definidas no pacote <a href="https://github.com/FluxML/NNlib.jl">FluxML/NNlib.jl</a> e utilizadas &#40;reexportadas&#41; pelo <code>Flux.jl</code>.</p>
</li>
<li><p>Podemos acessar os códigos de implementação dessas funções de ativação em <a href="https://github.com/FluxML/NNlib.jl/blob/master/src/activations.jl">NNlib.jl/src/activations.jl</a>.</p>
</li>
<li><p>A lista delas está acessível na constante <code>NNlib.ACTIVATIONS</code>:</p>
</li>
</ul>
<pre><code class="language-julia">NNlib.ACTIVATIONS</code></pre>
<pre><code class="language-julia">24-element Vector&#123;Symbol&#125;:
 :σ
 :hardσ
 :hardtanh
 :relu
 :leakyrelu
 :relu6
 :rrelu
 :elu
 :gelu
 :swish
 ⋮
 :logσ
 :logcosh
 :mish
 :tanhshrink
 :softshrink
 :trelu
 :lisht
 :tanh_fast
 :sigmoid_fast</code></pre>
<ul>
<li><p>Uma das mais conhecidas é a sigmoid:</p>
</li>
</ul>
\[ σ(x) = \frac{1}{1 + \exp(-x)}.
\]
<pre><code class="language-julia">plot&#40;-10:0.1:10, NNlib.σ, legend&#61;false,
    title&#61;&quot;Gráfico da sigmoid &#96;σ&#40;x&#41; &#61; 1 / &#40;1 &#43; exp&#40;-x&#41;&#41;&#96;&quot;, titlefont&#61;11&#41;</code></pre>
<img src="/modelagem_matematica/assets/pages/jupytered/c04/0406-Redes_neurais/code/images/0406-Redes_neurais_9_1.png" alt="">
<h2 id="visualizando_todas_as_funções_de_ativação"><a href="#visualizando_todas_as_funções_de_ativação" class="header-anchor">Visualizando todas as funções de ativação</a></h2>
<pre><code class="language-julia">ativacoes &#61; hcat&#40;&#91;getproperty&#40;NNlib, ativacao&#41;.&#40;-10:0.1:10&#41; for ativacao in NNlib.ACTIVATIONS&#91;1:end&#93;&#93;...&#41;
nothing</code></pre>
<pre><code class="language-julia">ncols &#61; 3
nlinhas &#61; divrem&#40;length&#40;NNlib.ACTIVATIONS&#41;, ncols&#41; |&gt; drn -&gt; drn&#91;1&#93; &#43; sign&#40;drn&#91;2&#93;&#41;
plot&#40;ativacoes, layout &#61; grid&#40;nlinhas, ncols&#41;, legend&#61;false, size&#61;&#40;600,1000&#41;,
    title&#61;hcat&#40;NNlib.ACTIVATIONS...&#41;, titlefont&#61;8&#41;</code></pre>
<img src="/modelagem_matematica/assets/pages/jupytered/c04/0406-Redes_neurais/code/images/0406-Redes_neurais_11_1.png" alt="">
<h2 id="outros_neurônios"><a href="#outros_neurônios" class="header-anchor">Outros neurônios</a></h2>
<ul>
<li><p>Podemos compor uma função de ativação \(f:\mathbb{R}\rightarrow \mathbb{R}\) qualquer...</p>
</li>
<li><p>... com a média ponderada \(w_1x_1 + \ldots + w_nx_n\) dos sinais de entrada ...</p>
</li>
<li><p>... para formar um neurônio</p>
</li>
</ul>
\[ (x_1, \ldots, x_n) \mapsto f(w_1x_1 + \ldots + w_nx_n).
\]
<ul>
<li><p>Isso pode ser feito explicitamente como acima.</p>
</li>
<li><p>Ou com o <code>Dense&#40;&#41;</code> do <code>Flux.jl</code>.</p>
</li>
</ul>
<pre><code class="language-julia">n &#61; 2
m &#61; Dense&#40;n, 1&#41;</code></pre>
<pre><code class="language-julia">Dense&#40;2 &#61;&gt; 1&#41;       # 3 parameters</code></pre>
<h2 id="visualização_da_rede"><a href="#visualização_da_rede" class="header-anchor">Visualização da rede</a></h2>
<ul>
<li><p>A visualização dessa rede pode ser feita com o pacote <a href="https://github.com/rmsrosa/ChainPlots.jl">ChainPlots.jl</a>.</p>
</li>
</ul>
<pre><code class="language-julia">plot&#40;m, size&#61;&#40;400, 200&#41;&#41;</code></pre>
<pre><code class="language-julia">Error: UndefVarError: Plots not defined</code></pre>
<h2 id="sobre_o_dense"><a href="#sobre_o_dense" class="header-anchor">Sobre o <code>Dense</code></a></h2>
<ul>
<li><p><code>Dense</code> é um conjunto de &quot;coisas&quot;, de acordo com a filosofia de <em>múltiplo despachos</em> do julia.</p>
</li>
<li><p><code>Dense</code> é um <strong>struct</strong> &#40;ou &quot;tipo composto&quot;&#41;, que armazena uma <em>matriz de pesos</em>, o <em>viés</em> e <em>função de ativação</em>, representando, assim, um tipo de neurônio.</p>
</li>
<li><p><code>Dense</code> são vários <strong>inner and outer constructors</strong> &#40;ou &quot;construtores internos e externos&quot;&#41;, que servem para criar uma instância do <em>struct</em> <code>Dense</code> de maneiras diferentes.</p>
</li>
<li><p><code>Dense</code> também acarreta na definição de um método que &quot;avalia&quot; a ação de uma instância do struct nos sinais de entrada &#40;a ação do neurônio em si&#41;.</p>
</li>
</ul>
<pre><code class="language-julia">fieldnames&#40;Dense&#41; # nomes dos campos do tipo composto</code></pre>
<pre><code class="language-julia">&#40;:weight, :bias, :σ&#41;</code></pre>
<pre><code class="language-julia">methods&#40;Dense&#41; # métodos para a construção do tipo composto</code></pre>
<pre><code class="language-julia"># 7 methods for type constructor:
&#91;1&#93; Flux.Dense&#40;in::Integer, out::Integer&#41; in Flux at /Users/rrosa/.julia/pa
ckages/Flux/6Q5r4/src/deprecations.jl:63
&#91;2&#93; Flux.Dense&#40;in::Integer, out::Integer, σ; kw...&#41; in Flux at /Users/rrosa
/.julia/packages/Flux/6Q5r4/src/deprecations.jl:63
&#91;3&#93; Flux.Dense&#40;::Pair&#123;&lt;:Integer, &lt;:Integer&#125;&#41; in Flux at /Users/rrosa/.julia
/packages/Flux/6Q5r4/src/layers/basic.jl:150
&#91;4&#93; Flux.Dense&#40;::Pair&#123;&lt;:Integer, &lt;:Integer&#125;, σ; init, bias&#41; in Flux at /Use
rs/rrosa/.julia/packages/Flux/6Q5r4/src/layers/basic.jl:150
&#91;5&#93; Flux.Dense&#40;W::M&#41; where M&lt;:&#40;AbstractMatrix&#41; in Flux at /Users/rrosa/.jul
ia/packages/Flux/6Q5r4/src/layers/basic.jl:144
&#91;6&#93; Flux.Dense&#40;W::M, bias&#41; where M&lt;:&#40;AbstractMatrix&#41; in Flux at /Users/rros
a/.julia/packages/Flux/6Q5r4/src/layers/basic.jl:144
&#91;7&#93; Flux.Dense&#40;W::M, bias, σ::F&#41; where &#123;M&lt;:&#40;AbstractMatrix&#41;, F&#125; in Flux at 
/Users/rrosa/.julia/packages/Flux/6Q5r4/src/layers/basic.jl:144</code></pre>
<pre><code class="language-julia">methods&#40;m&#41; # métodos definidos</code></pre>
<pre><code class="language-julia"># 2 methods:
&#91;1&#93; &#40;a::Flux.Dense&#41;&#40;x::AbstractVecOrMat&#41; in Flux at /Users/rrosa/.julia/pac
kages/Flux/6Q5r4/src/layers/basic.jl:157
&#91;2&#93; &#40;a::Flux.Dense&#41;&#40;x::AbstractArray&#41; in Flux at /Users/rrosa/.julia/packag
es/Flux/6Q5r4/src/layers/basic.jl:162</code></pre>
<pre><code class="language-julia">methodswith&#40;Dense&#41;</code></pre>
<pre><code class="language-julia">Error: UndefVarError: methodswith not defined</code></pre>
<pre><code class="language-julia">show&#40;Docs.doc&#40;Dense&#41;&#41;</code></pre>

<p>Dense&#40;in &#61;&gt; out, σ&#61;identity; bias&#61;true, init&#61;glorot_uniform&#41; Dense&#40;W::AbstractMatrix, &#91;bias, σ&#93;&#41;</p>
<pre><code class="language-julia">Create a traditional fully connected layer, whose forward pass is given by:</code></pre>
<p>y &#61; σ.&#40;W * x .&#43; bias&#41;</p>
<pre><code class="language-julia">The input &#96;x&#96; should be a vector of length &#96;in&#96;, or batch of vectors repres
ented as an &#96;in × N&#96; matrix, or any array with &#96;size&#40;x,1&#41; &#61;&#61; in&#96;. The out &#96;
y&#96; will be a vector  of length &#96;out&#96;, or a batch with &#96;size&#40;y&#41; &#61;&#61; &#40;out, siz
e&#40;x&#41;&#91;2:end&#93;...&#41;&#96;

Keyword &#96;bias&#61;false&#96; will switch off trainable bias for the layer. The init
ialisation of the weight matrix is &#96;W &#61; init&#40;out, in&#41;&#96;, calling the functio
n given to keyword &#96;init&#96;, with default &#91;&#96;glorot_uniform&#96;&#93;&#40;@doc Flux.glorot
_uniform&#41;. The weight matrix and/or the bias vector &#40;of length &#96;out&#96;&#41; may a
lso be provided explicitly.

# Examples

&#96;&#96;&#96;jldoctest
julia&gt; d &#61; Dense&#40;5 &#61;&gt; 2&#41;
Dense&#40;5 &#61;&gt; 2&#41;       # 12 parameters

julia&gt; d&#40;rand&#40;Float32, 5, 64&#41;&#41; |&gt; size
&#40;2, 64&#41;

julia&gt; d&#40;rand&#40;Float32, 5, 1, 1, 64&#41;&#41; |&gt; size  # treated as three batch dime
nsions
&#40;2, 1, 1, 64&#41;

julia&gt; d1 &#61; Dense&#40;ones&#40;2, 5&#41;, false, tanh&#41;  # using provided weight matrix
Dense&#40;5 &#61;&gt; 2, tanh; bias&#61;false&#41;  # 10 parameters

julia&gt; d1&#40;ones&#40;5&#41;&#41;
2-element Vector&#123;Float64&#125;:
 0.9999092042625951
 0.9999092042625951

julia&gt; Flux.params&#40;d1&#41;  # no trainable bias
Params&#40;&#91;&#91;1.0 1.0 … 1.0 1.0; 1.0 1.0 … 1.0 1.0&#93;&#93;&#41;
&#96;&#96;&#96;</code></pre>
<pre><code class="language-julia">@which Dense&#40;2,1&#41;</code></pre>
<pre><code class="language-julia">Error: LoadError: UndefVarError: @which not defined
in expression starting at /Users/rrosa/Documents/git_repositories/modelagem
_matematica/src/jupyter/c04/0406-Redes_neurais.ipynb:2</code></pre>
<ul>
<li><p>Código fonte para <code>Flux.Dense</code> em <a href="https://github.com/FluxML/Flux.jl/blob/master/src/layers/basic.jl#L71">src/basic.jl#L71</a>, no repositório do <code>Flux.jl</code>.</p>
</li>
</ul>
<h2 id="diferenças_sobre_a_nossa_implementação"><a href="#diferenças_sobre_a_nossa_implementação" class="header-anchor">Diferenças sobre a nossa implementação</a></h2>
<ul>
<li><p>A nossa definição foi</p>
</li>
</ul>
<pre><code class="language-julia">l&#40;x, h, W, b&#41; &#61; h.&#40;W * x .&#43; b&#41;</code></pre>
<ul>
<li><p>A definição para uma instância do <code>Dense</code> é &#40;obtido de <a href="https://github.com/FluxML/Flux.jl/blob/master/src/layers/basic.jl#L71">src/basic.jl#L71</a>&#41;</p>
</li>
</ul>
<pre><code class="language-julia">function &#40;a::Dense&#41;&#40;x::AbstractVecOrMat&#41;
  W, b, σ &#61; a.weight, a.bias, a.σ
  return σ.&#40;W*x .&#43; b&#41;
end</code></pre>
<ul>
<li><p><strong>Observação:</strong> No <em>struct</em> do dense, <code>σ</code> é o nome do campo que guarda a função de ativação do neurônio, que pode ser qualquer uma, não apenas a sigmóide <code>σ</code>, importada de <code>NNLib.σ</code>.</p>
</li>
</ul>
<h2 id="rede_pró-alimentada"><a href="#rede_pró-alimentada" class="header-anchor">Rede pró-alimentada</a></h2>
<ul>
<li><p>Em uma rede pró-alimentada, concatenamos uma série de camadas densas.</p>
</li>
<li><p>No <code>Flux.jl</code>, essa concatenação, ou composição, é feita com a função <code>Chain</code>.</p>
</li>
</ul>
<pre><code class="language-julia">m &#61; Chain&#40;Dense&#40;2, 4, σ&#41;, Dense&#40;4, 8, tanh&#41;, Dense&#40;8, 1, relu&#41;&#41;</code></pre>
<pre><code class="language-julia">Chain&#40;
  Dense&#40;2 &#61;&gt; 4, σ&#41;,                     # 12 parameters
  Dense&#40;4 &#61;&gt; 8, tanh&#41;,                  # 40 parameters
  Dense&#40;8 &#61;&gt; 1, relu&#41;,                  # 9 parameters
&#41;                   # Total: 6 arrays, 61 parameters, 628 bytes.</code></pre>
<pre><code class="language-julia">plot&#40;m, title &#61; &quot;&#36;m&quot;, titlefont &#61; 10&#41;</code></pre>
<pre><code class="language-julia">Error: UndefVarError: Plots not defined</code></pre>
<h2 id="outros_tipos_de_camadas"><a href="#outros_tipos_de_camadas" class="header-anchor">Outros tipos de camadas</a></h2>
<ul>
<li><p>Há vários tipos de camadas: recorrentes, convolucionais, <em>pooling</em>, etc.</p>
</li>
<li><p>Cada uma com as suas aplicações.</p>
</li>
<li><p>Redes recorrentes, por exemplo, guardam estados anteriores e são úteis onde a &quot;história recente&quot; de um dado é relevantes para o contexto, como em processamento de linguagem &#40;e.g. palavras seguidas dando sentido a uma frase&#41;.</p>
</li>
<li><p>Redes convolucionais fazem uma média ponderada de apenas algumas células vizinhas, importante quando as informações são localmente correlacionadas, como em processamento de imagens &#40;e.g. uma imagem 2D onde cada pixel está, em geral, correlacionado a vários pixels vizinhos&#41;.</p>
</li>
<li><p>Mas não vamos explorar isso a fundo. Isso é feito em cursos específicos de redes neurais. A ideia, aqui, é dar uma motivação geral e desmistificar um pouco isso.</p>
</li>
<li><p>No próximo caderno, vamos nos concentrar em redes densas pró-alimentadas e fazer alguns ajustes de dados sintéticos.</p>
</li>
</ul>

    <div class="navbar">
    <p id="nav">
<span id="nav-prev" style="float: left;">
<a class="menu-level-1" href="/modelagem_matematica/pages/jupytered/c04/0405-Exemplos_ajuste_naolinear">4.5. Exemplos de ajuste não-linear de parâmetros <kbd>←</kbd></a>
</span>
<span id="nav-next" style="float: right;">
    <a class="menu-level-1" href="/modelagem_matematica/pages/jupytered/c04/0407-Ajuste_em_redes_neurais"><kbd>→</kbd> 4.7. Ajuste de parâmetros em modelos de redes neurais</a>
</span>
    </p>
</div>
</br></br>



<div class="page-foot">
    
        <div class="license">
            <a href=https://creativecommons.org/licenses/by-nc-nd/4.0/>(CC BY-NC-ND 4.0) Attribution-NonCommercial-NoDerivatives 4.0 International </a>
            
        </div>
    

    Last modified: May 19, 2022. Built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>.
</div><!-- CONTENT ENDS HERE -->

      </div> <!-- .books-content -->
    </div> <!-- .books-container -->

    
        <script src="/modelagem_matematica/libs/katex/katex.min.js"></script>
        <script src="/modelagem_matematica/libs/katex/auto-render.min.js"></script>
        <script>renderMathInElement(document.body)</script>
    

    
        <script src="/modelagem_matematica/libs/highlight/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>
    

  </body>
</html>
